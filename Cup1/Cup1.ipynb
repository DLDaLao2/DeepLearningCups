{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from scipy.sparse import hstack\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/train.csv')\n",
    "X = df.loc[:, 'Page content'].to_numpy()\n",
    "y = df.loc[:,'Popularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'positive-words.txt')\n",
    "NEGATIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'negative-words.txt')\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "\n",
    "for line in open(POSITIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    pos_words.append(word)\n",
    "\n",
    "for line in open(NEGATIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    neg_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_neg(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in neg_words and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def tokenizer_stem_pos(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in pos_words and re.match('[a-zA-Z]+', w)]\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of mem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "batch_size = 1024\n",
    "iters = int((27643+batch_size-1)/(batch_size*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = get_stream(path='./datasets/train.csv', size=batch_size)\n",
    "idx = 0\n",
    "X_train, y_train = None, None\n",
    "batch_X, batch_y = None, None\n",
    "for z in range(iters):\n",
    "    batch = next(stream)\n",
    "    if(idx==0):\n",
    "        X_train= batch['Page content']\n",
    "        y_train = batch['Popularity']\n",
    "        idx+=1\n",
    "    else:\n",
    "        X_train = np.concatenate((X_train, batch['Page content']))\n",
    "        y_train = np.concatenate((y_train, batch['Popularity']))\n",
    "    batch = next(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "author_score =dict()\n",
    "author_num = dict()\n",
    "channel_score = dict()\n",
    "channel_num = dict()\n",
    "topic_score = dict()\n",
    "topic_num = dict()\n",
    "avg_author = 0\n",
    "avg_channel = 0\n",
    "avg_topic = 0\n",
    "\n",
    "def feature_selection_part2(data, istraining):\n",
    "    X = []\n",
    "    idx=0\n",
    "    global author_score\n",
    "    global author_num \n",
    "    global channel_score \n",
    "    global channel_num \n",
    "    global topic_score \n",
    "    global topic_num \n",
    "    global avg_author \n",
    "    global avg_channel \n",
    "    global avg_topic \n",
    "    \n",
    "    if(istraining):\n",
    "        for html in data:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            channel = soup.find(\"article\").get(\"data-channel\")\n",
    "            if channel in channel_score:\n",
    "                channel_score[channel] += 1 if int(y[idx])==1 else  0\n",
    "                channel_num[channel] += 1\n",
    "            else : \n",
    "                channel_score[channel] = 0\n",
    "                channel_score[channel] += 1 if int(y[idx])==1 else  0\n",
    "                channel_num[channel] = 1\n",
    "            \n",
    "            topics = soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\")\n",
    "            for topic in topics:\n",
    "                if topic in topic_score:\n",
    "                    topic_score[topic] += 1 if int(y[idx])==1 else  0\n",
    "                    topic_num[topic] += 1\n",
    "                else : \n",
    "                    topic_score[topic] = 0\n",
    "                    topic_score[topic] += 1 if int(y[idx])==1 else  0\n",
    "                    topic_num[topic] = 1\n",
    "\n",
    "\n",
    "            # Author\n",
    "            author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "            if soup.head.find(\"span\") == None:\n",
    "                continue\n",
    "            else:\n",
    "                author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "                if author in author_score:\n",
    "                    author_score[author] += 1 if int(y[idx])==1 else  0\n",
    "                    author_num[author] += 1\n",
    "                else : \n",
    "                    author_score[author] = 0\n",
    "                    author_num[author] = 1\n",
    "                    author_score[author] += 1 if int(y[idx])==1 else  0\n",
    "            idx+=1\n",
    "        total_channel = 0\n",
    "        total_topic = 0\n",
    "        total_author = 0\n",
    "        for c,s in channel_score.items():\n",
    "            avg_channel+=s\n",
    "            total_channel+=channel_num[c]\n",
    "        avg_channel = avg_channel/total_channel\n",
    "\n",
    "        for c,s in topic_score.items():\n",
    "            avg_topic+=s\n",
    "            total_topic+=topic_num[c]\n",
    "        avg_topic = avg_topic/total_topic\n",
    "\n",
    "        for c,s in author_score.items():\n",
    "            avg_author+=s\n",
    "            total_author+=author_num[c]\n",
    "        avg_author = avg_author/total_author\n",
    "\n",
    "    for html in data:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        feature_list = []\n",
    "        # Author score\n",
    "        author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "        if soup.head.find(\"span\") == None:\n",
    "            feature_list.append(avg_author)\n",
    "        else:\n",
    "            author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "            if author in author_score:\n",
    "                feature_list.append(author_score[author]/author_num[author])\n",
    "            else:\n",
    "                feature_list.append(avg_author)\n",
    "        # Channel score\n",
    "        channel = soup.find(\"article\").get(\"data-channel\")\n",
    "        if channel in channel_score:\n",
    "            feature_list.append(channel_score[channel]/channel_num[channel])\n",
    "        else:\n",
    "            feature_list.append(avg_channel)\n",
    "            \n",
    "        # Topic score\n",
    "        topics = soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\")\n",
    "        total_score = 0\n",
    "        order = 0 \n",
    "        order_denominator = 0\n",
    "        for i in range(len(topics)):\n",
    "            order_denominator += math.exp(-0.5*i)\n",
    "\n",
    "        for topic in topics:\n",
    "            order_coef = math.exp(-0.5*order)/order_denominator\n",
    "            if topic in topic_score:\n",
    "                total_score+=(topic_score[topic]/topic_num[topic])*order_coef\n",
    "            else:\n",
    "                total_score+=avg_topic*order_coef\n",
    "            order+=1\n",
    "        \n",
    "        feature_list.append(total_score) \n",
    "\n",
    "        # word count of title\n",
    "        feature_list.append(len(soup.find(\"h1\", {\"class\": \"title\"}).text))\n",
    "        # average word length and unique word rate\n",
    "        words = re.findall(r'\\w+', soup.get_text().lower())\n",
    "        if words:\n",
    "            total_words = len(words)\n",
    "            unique_words = set(words)\n",
    "            unique_word_count = len(unique_words)\n",
    "            total_length = sum(len(word) for word in words)\n",
    "            unique_rate = unique_word_count / total_words\n",
    "            average_length = total_length / len(words)\n",
    "            feature_list.append(unique_rate)\n",
    "            feature_list.append(average_length)\n",
    "        \n",
    "        # Time\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            month = int(re.search(r'(\\d+-\\d+-\\d+)', soup.time.text).group(1).split(\"-\")[1])\n",
    "            feature_list.append(month)\n",
    "            hour = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[0])\n",
    "            minute = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[1])\n",
    "            time = (hour * 60 + minute-300)%1440\n",
    "            time_demarcation = 0\n",
    "            if(300<=time and time<=720):\n",
    "                time_demarcation = 1\n",
    "            elif(720<time and time<=960):\n",
    "                time_demarcation = 2\n",
    "            elif(960<time and time <=1440):\n",
    "                time_demarcation = 3\n",
    "            else:\n",
    "                time_demarcation = 4\n",
    "            feature_list.append(time_demarcation)\n",
    "\n",
    "        # Weekend\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            feature_list.append(1 if soup.time.get(\"datetime\")[:3] in [\"Sat\", \"Sun\"] else 0)\n",
    "\n",
    "        # Word Count\n",
    "        text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "        c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "        text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "        c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "        total = c_div + c_p\n",
    "        #feature_list.append(len(list(total.elements())))\n",
    "\n",
    "        section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "\n",
    "        # Video + Image count\n",
    "        img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "        video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "        media_count = img_count + video_count\n",
    "        feature_list.append(media_count)\n",
    "\n",
    "        # Appealing count\n",
    "        link_count = len(section.find_all(\"a\"))\n",
    "        strong_count = len(section.find_all(\"strong\"))\n",
    "        appealing_count = link_count + strong_count\n",
    "        feature_list.append(appealing_count)\n",
    "\n",
    "        # POS & NEG count\n",
    "        paragraph = section.find_all(\"p\")\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        q_count = 0\n",
    "        ex_count = 0\n",
    "        para_count = 0\n",
    "        for tag in paragraph:\n",
    "            para_count += 1\n",
    "            pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "            neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "            if tag.text.find(\"?\") != -1:\n",
    "                q_count += tag.text.find(\"?\")\n",
    "            if tag.text.find(\"!\") != -1:\n",
    "                ex_count += tag.text.find(\"!\")\n",
    "        feature_list.append(pos_count)    \n",
    "        feature_list.append(neg_count)\n",
    "        #feature_list.append(q_count)\n",
    "        feature_list.append(ex_count)\n",
    "        feature_list.append(len(list(total.elements()))/para_count)\n",
    "\n",
    "        X.append(feature_list)\n",
    "    return np.array(X)\n",
    "# feature_selection_part2(X[0:10],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=.2)\n",
    "x_train = feature_selection_part2(X_train, True)\n",
    "x_valid = feature_selection_part2(X_valid, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [22114, 13312]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m Ridge\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lasso \u001b[39m=\u001b[39m Ridge()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lasso\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X62sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m indexes \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mauthor_score\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mchannel_score\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtopic_score\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle word count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39munique_rate\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m            \u001b[39m'\u001b[39m\u001b[39maverage_word_length\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmonth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtime_demarcation\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mweekend\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmedia_count\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X62sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m            , \u001b[39m'\u001b[39m\u001b[39mappealing_count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpos_cnt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneg_cnt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mex_cnt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mavg_para_word_cnt\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X62sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m coef \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(lasso\u001b[39m.\u001b[39mcoef_,index\u001b[39m=\u001b[39mindexes)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:1123\u001b[0m, in \u001b[0;36mRidge.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[39m\"\"\"Fit Ridge regression model.\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m \n\u001b[0;32m   1105\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m _accept_sparse \u001b[39m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[39m.\u001b[39missparse(X), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver)\n\u001b[1;32m-> 1123\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1124\u001b[0m     X,\n\u001b[0;32m   1125\u001b[0m     y,\n\u001b[0;32m   1126\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m_accept_sparse,\n\u001b[0;32m   1127\u001b[0m     dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[0;32m   1128\u001b[0m     multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1129\u001b[0m     y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1130\u001b[0m )\n\u001b[0;32m   1131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\utils\\validation.py:1164\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m-> 1164\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1166\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [22114, 13312]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "lasso = Ridge()\n",
    "lasso.fit(x_train, Y_train)\n",
    "indexes = ['author_score', 'channel_score', 'topic_score', 'title word count', 'unique_rate',\n",
    "           'average_word_length', 'month', 'time_demarcation', 'weekend', 'media_count'\n",
    "           , 'appealing_count', 'pos_cnt', 'neg_cnt', 'ex_cnt', 'avg_para_word_cnt']\n",
    "coef = pd.Series(lasso.coef_,index=indexes)\n",
    "print(coef.abs().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Word Training...\n",
      "Begin Evaluation...\n",
      "Stats train score: 0.7245622922066023\n",
      "Stats valid score: 0.5808389934528732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "# voter1 = RandomForestClassifier(n_estimators=400, max_depth=8, max_features=0.5)\n",
    "# voter2 = XGBClassifier()\n",
    "# voter3 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "# voting = VotingClassifier(estimators=[{'sgdc', voter1}, {'xgbc', voter2}, {'gbc', voter3}], voting='soft')\n",
    "model = RandomForestClassifier(n_estimators=400, max_depth=8, max_features=0.5)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "#x_train = feature_selection_part2(X_train, True)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)\n",
    "voting = model.fit(x_train, y_train)\n",
    "pkl.dump(model, open(\"output/backup_model.pickle\", \"wb\"))\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_pred_train = voting.predict_proba(x_train)[:,1]\n",
    "y_pred_valid = voting.predict_proba(x_valid)[:,1]\n",
    "\n",
    "train_score = roc_auc_score(y_train, y_pred_train)\n",
    "valid_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(f'Stats train score: {train_score}')\n",
    "print(f'Stats valid score: {valid_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      " Id                                                          27643\n",
      "Page content    <html><head><div class=\"article-info\"><span cl...\n",
      "Name: 0, dtype: object\n",
      "(11847,)\n",
      "\n",
      "Id:\n",
      " 27643\n",
      "Content:\n",
      " <html><head><div class=\"article-info\"><span class=\"byline \"><a href=\"/author/sam-laird/\"><img alt=\"2016%2f09%2f15%2f63%2fhttpsd2mhye01h4nj2n.cloudfront.netmediazgkymde1lza2.9814b\" class=\"author_image\" src=\"http://i.amz.mshcdn.com/-qaMPB8aiQeIaoBhqlU0OLjA07A=/90x90/2016%2F09%2F15%2F63%2Fhttpsd2mhye01h4nj2n.cloudfront.netmediaZgkyMDE1LzA2.9814b.jpg\"/></a><span class=\"author_name\">By <a href=\"/author/sam-laird/\">Sam Laird</a></span><time datetime=\"Mon, 09 Sep 2013 19:47:02 +0000\">2013-09-09 19:47:02 UTC</time></span></div></head><body><h1 class=\"title\">Soccer Star Gets Twitter Death Threats After Tackling One Direction Member</h1><figure class=\"article-image\"></figure><article data-channel=\"entertainment\"><section class=\"article-content\"> <div class=\"shift-to-hero\"> <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"360\" src=\"https://www.youtube.com/embed/8L0lUoJ2BCY?enablejsapi=1&amp;\" width=\"640\"></iframe></p> <script src=\"http://a.amz.mshcdn.com/assets/lib/aab-7ce243b38b9cc2caec816aff811d3153.js\" type=\"text/javascript\"></script> </div> <p>Note to humanity: One Direction fandom ain't nothin' to mess with. </p> <p>A British <a href=\"http://mashable.com/category/soccer/\">soccer</a> star learned this hard way over the weekend when the pop band's fanboys and fangirls brought a ruckus to his <a href=\"http://mashable.com/category/twitter/\">Twitter</a> feed, tweeting him a fusillade of death threats following an incidental run-in Sunday during a charity soccer match.</p> <div class=\"see-also\"><p>See also: <a href=\"http://mashable.com/2013/07/30/gq-one-direction-death-threats/\">GQ Gets Twitter Death Threats for One Direction Magazine Covers</a></p></div> <p>Gabriel Agbonlahor is the all-time leading goal-scorer for English Premier League side Aston Villa. He participated in a charity match honoring a former player now battling leukemia. Also playing: Louis Tomlinson, one-fifth of the British-Irish boy band <a href=\"http://mashable.com/category/one-direction/\">One Direction</a>. </p> <p>Tomlinson was dribbling the ball about midway through Sunday's match when the pro striker Agbonlahor swooped in to knock it away from the tiny amateur. A clean play, to be sure, but there was some incidental contact as well, causing Tomlinson to collapse on the grass like a wounded deer. Then he limped off and promptly vomited into his hand, <a href=\"http://www.youtube.com/watch?v=19AADH8tcBM\" target=\"_blank\">as shown here</a>. </p> <p>Tomlinson and Agbonlahor shared a moment as he left the field and all seemed fine on the pitch. On Twitter, however, things were just getting started: One Direction's fan base exploded in a fit of mass rage, threatening to end Agbonlahor in all sorts of unpleasant ways. To whit, here are just a few examples, with some mildly NSFW language:</p> <blockquote class=\"twitter-tweet\"> <p>WHOEVER PUSHED LOUIS DOWN I WILL FIND YOU AND PUSH YOU OFF THE GODDAMN EMPIRE STATE BUILDING!</p> <p>— J a m R e y e s™ (@ItsJamReyes) <a href=\"https://twitter.com/ItsJamReyes/statuses/376714629201674241\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <blockquote class=\"twitter-tweet\"> <p>this is how Louis hurt his knee, I will find that guy, and kill him. <a href=\"https://t.co/3tbsGCT1bq\" target=\"_blank\">https://t.co/3tbsGCT1bq</a></p> <p>— PROUD OF LOUIS (@LatestOf1D) <a href=\"https://twitter.com/LatestOf1D/statuses/376715533560733696\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <blockquote class=\"twitter-tweet\"> <p><a href=\"https://twitter.com/gabby_10\" target=\"_blank\">@gabby_10</a> You obviously need Jesus. WE WILL FUCKING FIND YOU AND KILL YOU.</p> <p>— morgan (@NiallUrBasicUgh) <a href=\"https://twitter.com/NiallUrBasicUgh/statuses/376756036389982208\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <blockquote class=\"twitter-tweet\"> <p>fuck off you <a href=\"https://twitter.com/gabby_10\" target=\"_blank\">@gabby_10</a>, how could u touch my precious LOUIS? I WILL KILL U MUTHERFUCKER</p> <p>— Lucho García #51 (@Lois7x) <a href=\"https://twitter.com/Lois7x/statuses/376781833716760576\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <p>At least one person even took things a step further, threatening indiscriminate mass murder:</p> <blockquote class=\"twitter-tweet\"> <p>If i ever see louis cry bc he's hurt or sad i will kill the entire human race by myself</p> <p>— karol(arry) (@nuttelarry) <a href=\"https://twitter.com/nuttelarry/statuses/377046130682761216\" target=\"_blank\">September 9, 2013</a></p> </blockquote> <p>This is actually just the latest incident of rabid teenyboppers attacking <a href=\"http://mashable.com/sports/\">sports</a> stars online over perceived slights against their demigod idols of auto-tuned worship, however. </p> <p>If you're scoring at home, for example, you'll recall the time not so long ago that Justin Bieber fans <a href=\"http://mashable.com/2013/05/30/justin-bieber-fans-twitter-eric-dickerson/\">unleashed a digital fury</a> upon an NFL Hall of Famer following criticism of Bieber's alleged reckless driving. But the beef goes both ways, too. Hockey fans on Twitter <a href=\"http://mashable.com/2013/07/09/justin-bieber-stabley-cup/\">flew into a collective rage</a> in July after photos surfaced of Bieber posing in quintessentially Bieber-like fashion with the Stanley Cup. </p> <p><strong><div class=\"bonus-content\">BONUS: <a href=\"http://mashable.com/2013/08/04/one-direction-bullying-office-depot-school-supplies/\">One Direction and Directioners Fuse Online Powers to Battle Bullying</a> </div></strong></p> <section class=\"gallery\" data-display-mode=\"gallery\" data-id=\"11859\" data-slide-title-pos=\"\" data-slug=\"one-directions-anti-bullying-campaign-with-office-depot\"> <header> <h1>One Direction's Anti-Bullying Campaign With Office Depot</h1> </header> <ol class=\"slides\"> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced00048d\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/Un3ZihG1VmZWnvs-VprxzwDgFOI=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2F0Fr6eIUoB6I.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/0Fr6eIUoB6I\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">1. 1D + OD Together Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced00048e\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/oOrgEGOY7kCyASO2sJEuiJmcH4Y=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2F2RuwAne1GRk.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/2RuwAne1GRk\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">2. Harry Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced00048f\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/slMRLhuwNWBmi2WUWOsoChIkbT4=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2FIVQeItw5NHQ.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/IVQeItw5NHQ\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">3. Zayn Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced000490\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/eVdXXGFMQcb8Dd0CZ95pp3etjs4=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2FL6Wdbq6frxY.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/L6Wdbq6frxY\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">4. Niall Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced000491\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/-IEKxbwTTehrbTiUHsXA4gv3L4c=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2F6mvTafur08w.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/6mvTafur08w\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">5. Louis Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced000492\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/U0yZuGwxU2fTisly0txtcIBENkg=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2FUlg966fRbJQ.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/Ulg966fRbJQ\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">6. Liam Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> </ol> </section> <p><em>Image: Ian Gavan/Getty Images for Sony Pictures</em></p> <script>      window._msla=window.loadScriptAsync||function(src,id){if(document.getElementById(id))return;var js=document.createElement('script');js.id=id;js.src=src;document.getElementsByTagName('script')[0].parentNode.insertBefore(js,fjs);}; _msla(\"//platform.twitter.com/widgets.js\",\"twitter_jssdk\");</script> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/entertainment/\">Entertainment</a>, <a href=\"/category/music/\">Music</a>, <a href=\"/category/one-direction/\">One Direction</a>, <a href=\"/category/soccer/\">soccer</a>, <a href=\"/category/sports/\">Sports</a> </footer></body></html>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/test.csv')\n",
    "print(\"Content:\\n\",df.loc[0])\n",
    "\n",
    "Id = df.loc[:, 'Id'].to_numpy()\n",
    "X_test = df.loc[:, 'Page content'].to_numpy()\n",
    "print(X_test.shape)\n",
    "print(\"\\nId:\\n\", Id[0])\n",
    "print(\"Content:\\n\", X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# x_test_type = tfidf_type.transform(X_test)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x_test_stats \u001b[39m=\u001b[39m feature_selection_part2(X_test, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pred_model \u001b[39m=\u001b[39m pkl\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput/588RF.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_pred_stats_test \u001b[39m=\u001b[39m pred_model\u001b[39m.\u001b[39mpredict_proba(x_test_stats)[:,\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m paragraph:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m     para_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m     pos_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer_stem_pos(tag\u001b[39m.\u001b[39;49mtext))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m     neg_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer_stem_neg(tag\u001b[39m.\u001b[39mtext))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m     \u001b[39mif\u001b[39;00m tag\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenizer_stem_pos\u001b[39m(text):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     stemmer \u001b[39m=\u001b[39m LancasterStemmer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [stemmer\u001b[39m.\u001b[39;49mstem(w) \u001b[39mfor\u001b[39;49;00m w \u001b[39min\u001b[39;49;00m re\u001b[39m.\u001b[39;49msplit(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms+\u001b[39;49m\u001b[39m'\u001b[39;49m, text\u001b[39m.\u001b[39;49mstrip()) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39mif\u001b[39;49;00m w \u001b[39min\u001b[39;49;00m pos_words \u001b[39mand\u001b[39;49;00m re\u001b[39m.\u001b[39;49mmatch(\u001b[39m'\u001b[39;49m\u001b[39m[a-zA-Z]+\u001b[39;49m\u001b[39m'\u001b[39;49m, w)]\n",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenizer_stem_pos\u001b[39m(text):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     stemmer \u001b[39m=\u001b[39m LancasterStemmer()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [stemmer\u001b[39m.\u001b[39mstem(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m re\u001b[39m.\u001b[39msplit(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+\u001b[39m\u001b[39m'\u001b[39m, text\u001b[39m.\u001b[39mstrip()) \\\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39mif\u001b[39;00m w \u001b[39min\u001b[39;00m pos_words \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39mmatch(\u001b[39m'\u001b[39m\u001b[39m[a-zA-Z]+\u001b[39m\u001b[39m'\u001b[39m, w)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_test_stats = feature_selection_part2(X_test, False)\n",
    "pred_model = pkl.load(open(\"output/backup_model.pickle\", \"rb\"))\n",
    "\n",
    "y_pred_stats_test = pred_model.predict_proba(x_test_stats)[:,1]\n",
    "y_pred = np.around(y_pred_stats_test, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {'Id': Id, 'Popularity': y_pred}\n",
    "output_dataframe = pd.DataFrame(output_data)\n",
    "print(output_dataframe)\n",
    "\n",
    "output_dataframe.to_csv(\"./datasets/y_pred.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
