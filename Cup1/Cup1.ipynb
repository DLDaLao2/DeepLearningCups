{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./datasets/train.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label:\\n\", df.loc[0, 'Popularity'])\n",
    "print(\"Content:\\n\",df.loc[0,'Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, 'Page content'].to_numpy()\n",
    "y = df.loc[:,'Popularity'].to_numpy()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"Label:\\n\", y_train[0])\n",
    "print(\"Content:\\n\", X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = r'(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "\n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub(r'[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(X_train[0], 'html.parser')\n",
    "# soup = BeautifulSoup(X_train[0], 'html.parser')\n",
    "print(soup.prettify())\n",
    "# beautiful soup tools\n",
    "print(\"Title: \", soup.find(\"h1\", {\"class\": \"title\"}).text)\n",
    "print(\"Time: \", re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1))\n",
    "print(\"Data Channel: \", soup.find(\"article\").get(\"data-channel\"))\n",
    "print(\"Author: \", soup.find(\"span\", {\"class\": \"author_name\"}).text.replace(\"By \", \"\"))\n",
    "# print(\"Author: \", soup.find(\"span\").text)\n",
    "# print(\"Author: \", soup.find(\"a\").text)\n",
    "# print(\"Data Labels: \", soup.find('div', {'class': 'content-mash-video'}).get('data-labels'))\n",
    "# print(\"Data Title: \", soup.find(\"div\", {\"class\": 'content-mash-video'}).get('data-title'))\n",
    "print(soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "def feature_selection(data):\n",
    "    feature_list = []\n",
    "    idx = 0\n",
    "    for html_text in data:\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        feature_map = {}  \n",
    "        feature_map[\"Title\"] = soup.find(\"h1\", {\"class\": \"title\"}).text\n",
    "        # Author formate may various from news website\n",
    "        author = soup.find(\"span\", {\"class\": \"author_name\"}) # default format\n",
    "        if(author != None): \n",
    "            feature_map[\"Author\"] = author.text.replace(\"By \", \"\")\n",
    "        elif(soup.find(\"span\") != None): \n",
    "            feature_map[\"Author\"] = soup.find(\"span\").text\n",
    "        elif(soup.find(\"a\") != None): \n",
    "            feature_map[\"Author\"] = soup.find(\"a\").text\n",
    "        else: print(idx) # To check the undefine formate\n",
    "        feature_map[\"Time\"] = re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)\n",
    "        feature_map[\"Channel\"] = soup.find(\"article\").get(\"data-channel\")\n",
    "        feature_map[\"Topics\"] = soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\")\n",
    "         # Word Count\n",
    "        text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "        c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "        text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "        c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "        total = c_div + c_p\n",
    "        print(\"Total words: \", len(list(total.elements())))\n",
    "\n",
    "\n",
    "        section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "\n",
    "        # Video + Image count\n",
    "        img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "        video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "        media_count = img_count + video_count\n",
    "        print(\"Media count: \", media_count)\n",
    "\n",
    "        # Appealing count\n",
    "        link_count = len(section.find_all(\"a\"))\n",
    "        strong_count = len(section.find_all(\"strong\"))\n",
    "        appealing_count = link_count + strong_count\n",
    "        print(\"Link count: \", appealing_count)\n",
    "        # Check if there has any None value feature\n",
    "        if(feature_map[\"Title\"] == None): print(f\"Title: {idx}\")\n",
    "        if(feature_map[\"Author\"] == None): print(f\"Author: {idx}\")\n",
    "        if(feature_map[\"Time\"] == None): print(f\"Time: {idx}\")\n",
    "        if(feature_map[\"Channel\"] == None): print(f\"Channel: {idx}\")\n",
    "        if(feature_map[\"Topics\"] == None): print(f\"Topics: {idx}\")\n",
    "        feature_list.append(feature_map)\n",
    "        idx+=1\n",
    "    return feature_list \n",
    "\n",
    "print(feature_selection(X_valid)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(text):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    feature_str += soup.find(\"h1\", {\"class\": \"title\"}).text + \" \"\n",
    "    # Author formate may various from news website\n",
    "    author = soup.find(\"span\", {\"class\": \"author_name\"}) # default format\n",
    "    if(author != None): \n",
    "        feature_str += author.text.replace(\"By \", \"\") + \" \"\n",
    "    elif(soup.find(\"span\") != None): \n",
    "        feature_str += soup.find(\"span\").text + \" \"\n",
    "    elif(soup.find(\"a\") != None): \n",
    "        feature_str += soup.find(\"a\").text + \" \"\n",
    "    if(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text) == None):\n",
    "        feature_str += \"\"\n",
    "    else: feature_str += re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1) + \" \"\n",
    "    feature_str += soup.find(\"article\").get(\"data-channel\") + \" \"\n",
    "    feature_str += soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").replace(\",\", \"\")\n",
    "    \n",
    "    return feature_str\n",
    "\n",
    "print(feature_selection(X_valid[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop(feature_selection(X_valid[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "tfidf.fit(X_train)\n",
    "\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    # When sklearn version <= 0.24.x, should use get_feature_names()\n",
    "    # When sklearn version >= 1.0.x, should use get_feature_names_out()\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names_out()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "\n",
    "# doc_tfidf = tfidf.transform(X_train).toarray()\n",
    "# tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "# print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "# for tok, v in zip(tfidf.inverse_transform(np.ones((1, tfidf_sum.shape[0])))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "#                         np.sort(tfidf_sum)[::-1][:top]):\n",
    "#     print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation]\n",
    "    shuffled_Y = Y[permutation]\n",
    "    \n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = SGDClassifier(loss='log_loss', alpha=0.01, max_iter=100, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc, val_auc = [], []\n",
    "iters = 3\n",
    "batch_size = 512\n",
    "classes = np.array([-1, 1])\n",
    "\n",
    "# train_tfidf = tfidf.transform(X_train).toarray()\n",
    "# valid_tfidf = tfidf.transform(X_valid).toarray()\n",
    "\n",
    "for i in range(iters):\n",
    "    print(f\"epoch: {i+1}/{iters}\")\n",
    "    train_batches = random_mini_batches(X_train, y_train, batch_size)\n",
    "    valid_batches = random_mini_batches(X_valid, y_valid, int(batch_size * 0.2))\n",
    "    idx = 0\n",
    "    for train_batch in train_batches:\n",
    "        x_batch, y_batch = train_batch\n",
    "        x_batch = tfidf.transform(x_batch)\n",
    "        clf.partial_fit(x_batch, y_batch, classes=classes)\n",
    "        train_score = roc_auc_score(y_batch, clf.predict_proba(x_batch)[:,1])\n",
    "        train_auc.append(train_score)\n",
    "        print(f'[{(idx+1)*batch_size}/{X_train.shape[0]}]')\n",
    "        print(f'Train score: {train_score}')\n",
    "        \n",
    "        x_batch, y_batch = valid_batches[idx]\n",
    "        valid_score = roc_auc_score(y_batch, clf.predict_proba(tfidf.transform(x_batch))[:,1])\n",
    "        val_auc.append(valid_score)\n",
    "        print(f'Valid score: {valid_score}')\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('output'):\n",
    "    os.mkdir('output')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(train_auc)+1), train_auc, color='blue', label='Train auc')\n",
    "plt.plot(range(1, len(train_auc)+1), val_auc, color='red', label='Val auc')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('#Batches')\n",
    "plt.ylabel('Auc')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_score = roc_auc_score(y_valid, clf.predict_proba(tfidf.transform(X_valid))[:,1])\n",
    "print(f'Valid score: {valid_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict_proba(tfidf.transform(X_train[:10]))[:10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# loss='log' gives logistic regression\n",
    "# sklearn version > 1.0 should use log_loss\n",
    "clf = SGDClassifier(loss='log_loss', max_iter=100, tol=1e-3)\n",
    "hashvec = HashingVectorizer(n_features=2**20,\n",
    "                            preprocessor=feature_selection, tokenizer=tokenizer_stem_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "classes = np.array([-1, 1])\n",
    "train_auc, val_auc = [], []\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = 3\n",
    "\n",
    "for i in range(iters):\n",
    "    print(f\"epoch: {i+1}/{iters}\")\n",
    "    train_batches = random_mini_batches(X_train, y_train, batch_size)\n",
    "    valid_batches = random_mini_batches(X_valid, y_valid, int(batch_size * 0.2))\n",
    "    idx = 0\n",
    "    for train_batch in train_batches:\n",
    "        x_batch, y_batch = train_batch\n",
    "        x_batch = hashvec.transform(x_batch)\n",
    "        clf.partial_fit(x_batch, y_batch, classes=classes)\n",
    "        train_score = roc_auc_score(y_batch, clf.predict_proba(x_batch)[:,1])\n",
    "        train_auc.append(train_score)\n",
    "        print(f'[{(idx+1)*batch_size}/{X_train.shape[0]}]')\n",
    "        print(f'Train score: {train_score}')\n",
    "        \n",
    "        x_batch, y_batch = valid_batches[idx]\n",
    "        valid_score = roc_auc_score(y_batch, clf.predict_proba(hashvec.transform(x_batch))[:,1])\n",
    "        val_auc.append(valid_score)\n",
    "        print(f'Valid score: {valid_score}')\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('output'):\n",
    "    os.mkdir('output')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(train_auc)+1), train_auc, color='blue', label='Train auc')\n",
    "plt.plot(range(1, len(train_auc)+1), val_auc, color='red', label='Val auc')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('#Batches')\n",
    "plt.ylabel('Auc')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_score = roc_auc_score(y_valid, clf.predict_proba(hashvec.transform(X_valid))[:,1])\n",
    "print(f'Valid score: {valid_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict_proba(hashvec.transform(X_train[:10]))[:10,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/test.csv')\n",
    "print(\"Content:\\n\",df.loc[0])\n",
    "\n",
    "Id = df.loc[:, 'Id'].to_numpy()\n",
    "X_test = df.loc[:, 'Page content'].to_numpy()\n",
    "print(X_test.shape)\n",
    "print(\"\\nId:\\n\", Id[0])\n",
    "print(\"Content:\\n\", X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.around(clf.predict_proba(hashvec.transform(X_test))[:,1], decimals=1)\n",
    "print(y_pred.shape)\n",
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {'Id': Id, 'Popularity': y_pred}\n",
    "output_dataframe = pd.DataFrame(output_data)\n",
    "print(output_dataframe)\n",
    "\n",
    "output_dataframe.to_csv(\"./datasets/y_pred.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
