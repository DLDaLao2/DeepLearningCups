{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from scipy.sparse import hstack\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/train.csv')\n",
    "X = df.loc[:, 'Page content'].to_numpy()\n",
    "y = df.loc[:,'Popularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Features Extractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_author(text):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    # Author formate may various from news website\n",
    "    author = soup.find(\"span\", {\"class\": \"author_name\"}) # default format\n",
    "    if(author != None): \n",
    "        feature_str += author.text.replace(\"By \", \"\") + \" \"\n",
    "    elif(soup.find(\"span\") != None): \n",
    "        feature_str += soup.find(\"span\").text + \" \"\n",
    "    elif(soup.find(\"a\") != None): \n",
    "        feature_str += soup.find(\"a\").text + \" \"\n",
    "    feature_str = re.sub(r'[\\W]+', ' ', feature_str.lower())\n",
    "    return feature_str\n",
    "\n",
    "def feature_selection_titles(text):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    feature_str += soup.find(\"h1\", {\"class\": \"title\"}).text + \" \"\n",
    "    feature_str = re.sub(r'[\\W]+', ' ', feature_str.lower())\n",
    "    return feature_str\n",
    "\n",
    "def feature_selection_channels(text):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    feature_str += soup.find(\"article\").get(\"data-channel\") + \" \"\n",
    "    feature_str = re.sub(r'[\\W]+', ' ', feature_str.lower())\n",
    "    return feature_str\n",
    "\n",
    "def feature_selection_topic(text):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    feature_str += soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").replace(\",\", \"\")\n",
    "    feature_str = re.sub(r'[\\W]+', ' ', feature_str.lower())\n",
    "    return feature_str\n",
    "\n",
    "def extract_weekend(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    ret = 0\n",
    "    if soup.time.text == None or soup.time.text == \"\":\n",
    "        ret = 0\n",
    "    else:\n",
    "        if soup.time.get(\"datetime\")[:3] in [\"Sat\", \"Sun\"]:\n",
    "            ret = 1 \n",
    "        else:\n",
    "            ret = 0\n",
    "    return ret\n",
    "\n",
    "def extract_word_count(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    # Word Count\n",
    "    text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "    c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "    text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "    c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "    total = c_div + c_p\n",
    "    return len(list(total.elements()))\n",
    "\n",
    "def extract_media_count(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "    img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "    video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "    media_count = img_count + video_count\n",
    "    return media_count\n",
    "\n",
    "def extract_appealing_count(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "    link_count = len(section.find_all(\"a\"))\n",
    "    strong_count = len(section.find_all(\"strong\"))\n",
    "    appealing_count = link_count + strong_count\n",
    "    return appealing_count\n",
    "\n",
    "print(feature_selection_topic(X[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'positive-words.txt')\n",
    "NEGATIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'negative-words.txt')\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "\n",
    "for line in open(POSITIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    pos_words.append(word)\n",
    "\n",
    "for line in open(NEGATIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    neg_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_neg(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in neg_words and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def tokenizer_stem_pos(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in pos_words and re.match('[a-zA-Z]+', w)]\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of mem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "batch_size = 1024\n",
    "iters = int((27643+batch_size-1)/(batch_size*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = get_stream(path='./datasets/train.csv', size=batch_size)\n",
    "idx = 0\n",
    "X_train, y_train = None, None\n",
    "batch_X, batch_y = None, None\n",
    "for z in range(iters):\n",
    "    batch = next(stream)\n",
    "    if(idx==0):\n",
    "        X_train= batch['Page content']\n",
    "        y_train = batch['Popularity']\n",
    "        idx+=1\n",
    "    else:\n",
    "        X_train = np.concatenate((X_train, batch['Page content']))\n",
    "        y_train = np.concatenate((y_train, batch['Popularity']))\n",
    "    batch = next(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_author = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection_author,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "tfidf_channel = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection_channels,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "tfidf_title = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection_titles,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "tfidf_topic = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection_topic,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "# tfidf_author.fit(X_train)\n",
    "# #tfidf_channel.fit(X_train)\n",
    "# tfidf_title.fit(X_train)\n",
    "# tfidf_topic.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# hash words to 1024 buckets\n",
    "hashvec_author = HashingVectorizer(n_features=2**10,\n",
    "                            preprocessor=feature_selection_author,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "hashvec_channel = HashingVectorizer(n_features=2**10,\n",
    "                            preprocessor=feature_selection_channels,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "hashvec_title = HashingVectorizer(n_features=2**10,\n",
    "                            preprocessor=feature_selection_titles,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "hashvec_topic = HashingVectorizer(n_features=2**10,\n",
    "                            preprocessor=feature_selection_topic,\n",
    "                            tokenizer=tokenizer_stem_nostop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Feature Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(x_batch):\n",
    "    x_batch_channel = \"\"\n",
    "    x_batch_title = tfidf_title.transform(x_batch).toarray()\n",
    "    #print(x_batch_title.shape)\n",
    "    x_batch_author = tfidf_author.transform(x_batch).toarray()\n",
    "    #print(x_batch_author.shape)\n",
    "    x_batch_topic = tfidf_topic.transform(x_batch).toarray()\n",
    "    #print(x_batch_topic.shape)\n",
    "    # x_batch_channel = tfidf_channel.transform(x_batch).toarray()\n",
    "    #print(x_batch_channel.shape)\n",
    "    weekend_vectorized = np.vectorize(extract_weekend)\n",
    "    x_batch_weekend = weekend_vectorized(x_batch).reshape(len(x_batch),1)\n",
    "    #print(x_batch_weekend.shape)\n",
    "    media_vectorized = np.vectorize(extract_media_count)\n",
    "    x_batch_media = media_vectorized(x_batch).reshape(len(x_batch),1)\n",
    "    #print(x_batch_media.shape)\n",
    "    appealing_count_vectorized = np.vectorize(extract_appealing_count)\n",
    "    x_batch_appealing = appealing_count_vectorized(x_batch).reshape(len(x_batch),1)\n",
    "    #print(x_batch_appealing.shape)\n",
    "    word_vectorized = np.vectorize(extract_word_count)\n",
    "    x_batch_word = word_vectorized(x_batch).reshape(len(x_batch),1)\n",
    "    #print(x_batch_word.shape)\n",
    "    # combined_x_batch = np.concatenate((x_batch_title, x_batch_author, x_batch_channel, x_batch_topic),axis=1)\n",
    "    # , x_batch_weekend\n",
    "    #                            , x_batch_media, x_batch_appealing, x_batch_word\n",
    "    #print(combined_x_batch.shape)\n",
    "    return x_batch_title, x_batch_author, x_batch_channel, x_batch_topic, x_batch_weekend, x_batch_media, x_batch_appealing, x_batch_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasas grand challenge stop asteroids from destroying earth world moskowitz improv pref confid win support benefit valu integr win support import accompl innov tim kil dang threats asteroid asteroids challenge earth space us world \n"
     ]
    }
   ],
   "source": [
    "def feature_selection_part1(data):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    feature_str += soup.find(\"h1\", {\"class\": \"title\"}).text + \" \"\n",
    "\n",
    "    # Channel\n",
    "    feature_str += soup.find(\"article\").get(\"data-channel\") + \" \"\n",
    "\n",
    "    # Author\n",
    "    author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "    if soup.head.find(\"span\") == None:\n",
    "        feature_str += \" \"\n",
    "    else:\n",
    "        author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "        feature_str += author.split(\" \")[-1] + \" \"\n",
    "\n",
    "    section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "    paragraph = section.find_all(\"p\")\n",
    "    pos_count = 0\n",
    "    for tag in paragraph:\n",
    "        pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "        for word in tokenizer_stem_pos(tag.text):\n",
    "            feature_str += word + \" \"\n",
    "\n",
    "    neg_count = 0\n",
    "    for tag in paragraph:\n",
    "        neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "        for word in tokenizer_stem_neg(tag.text):\n",
    "            feature_str += word + \" \"\n",
    "\n",
    "    # Related Topics\n",
    "    feature_str += soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").replace(\",\", \"\")\n",
    "    \n",
    "    feature_str = re.sub(r'[.:\\']', '', feature_str.lower())\n",
    "    return feature_str\n",
    "print(feature_selection_part1(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "2\n",
      "0\n",
      "0\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "1\n",
      "-2\n",
      "-1\n",
      "2\n",
      "-1\n",
      "0\n",
      "0\n",
      "-1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "-1\n",
      "-2\n",
      "1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-2\n",
      "1\n",
      "-1\n",
      "1\n",
      "3\n",
      "-1\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "3\n",
      "-2\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "0\n",
      "-2\n",
      "1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "0\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "2\n",
      "-1\n",
      "0\n",
      "-2\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.00000000e+00,  6.00000000e+01,  4.89731438e-01, ...,\n",
       "         3.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 2.00000000e+00,  7.40000000e+01,  4.74719101e-01, ...,\n",
       "         3.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  6.80000000e+01,  4.98765432e-01, ...,\n",
       "         2.10000000e+01,  7.05000000e+02,  3.80000000e+01],\n",
       "       ...,\n",
       "       [ 0.00000000e+00,  4.10000000e+01,  4.49786325e-01, ...,\n",
       "         1.40000000e+01,  7.85000000e+02,  5.24000000e+02],\n",
       "       [ 0.00000000e+00,  5.90000000e+01,  5.12362637e-01, ...,\n",
       "         3.00000000e+00,  5.60000000e+01,  0.00000000e+00],\n",
       "       [ 1.00000000e+00,  5.80000000e+01,  6.45522388e-01, ...,\n",
       "         0.00000000e+00,  2.05000000e+02,  0.00000000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_selection_part2(data):\n",
    "    X = []\n",
    "    idx=0\n",
    "    author_score =dict()\n",
    "    for html in data:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        # Author\n",
    "        author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "        if soup.head.find(\"span\") == None:\n",
    "            continue\n",
    "        else:\n",
    "            author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "            if author in author_score:\n",
    "                author_score[author]+=int(y[idx])\n",
    "            else : \n",
    "                author_score[author] = 0\n",
    "                author_score[author]+=int(y[idx])\n",
    "        idx+=1\n",
    "    for html in data:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        feature_list = []\n",
    "        # Author score\n",
    "        author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "        if soup.head.find(\"span\") == None:\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "            feature_list.append(author_score[author])\n",
    "            print(author_score[author])\n",
    "\n",
    "        # word count of title\n",
    "        feature_list.append(len(soup.find(\"h1\", {\"class\": \"title\"}).text))\n",
    "        # average word length and unique word rate\n",
    "        words = re.findall(r'\\w+', soup.get_text().lower())\n",
    "        if words:\n",
    "            total_words = len(words)\n",
    "            unique_words = set(words)\n",
    "            unique_word_count = len(unique_words)\n",
    "            total_length = sum(len(word) for word in words)\n",
    "            unique_rate = unique_word_count / total_words\n",
    "            average_length = total_length / len(words)\n",
    "            feature_list.append(unique_rate)\n",
    "            feature_list.append(average_length)\n",
    "        \n",
    "        # Time\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            month = int(re.search(r'(\\d+-\\d+-\\d+)', soup.time.text).group(1).split(\"-\")[1])\n",
    "            feature_list.append(month)\n",
    "            hour = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[0])\n",
    "            minute = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[1])\n",
    "            feature_list.append(hour * 60 + minute)\n",
    "\n",
    "        # Weekend\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            feature_list.append(1 if soup.time.get(\"datetime\")[:3] in [\"Sat\", \"Sun\"] else 0)\n",
    "\n",
    "        # Word Count\n",
    "        text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "        c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "        text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "        c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "        total = c_div + c_p\n",
    "        feature_list.append(len(list(total.elements())))\n",
    "\n",
    "        section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "\n",
    "        # Video + Image count\n",
    "        img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "        video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "        media_count = img_count + video_count\n",
    "        feature_list.append(media_count)\n",
    "\n",
    "        # Appealing count\n",
    "        link_count = len(section.find_all(\"a\"))\n",
    "        strong_count = len(section.find_all(\"strong\"))\n",
    "        appealing_count = link_count + strong_count\n",
    "        feature_list.append(appealing_count)\n",
    "\n",
    "        # POS & NEG count\n",
    "        paragraph = section.find_all(\"p\")\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        q_count = 0\n",
    "        ex_count = 0\n",
    "        for tag in paragraph:\n",
    "            pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "            neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "            if tag.text.find(\"?\") != -1:\n",
    "                q_count += tag.text.find(\"?\")\n",
    "            if tag.text.find(\"!\") != -1:\n",
    "                ex_count += tag.text.find(\"!\")\n",
    "        feature_list.append(pos_count)    \n",
    "        feature_list.append(neg_count)\n",
    "        feature_list.append(q_count)\n",
    "        feature_list.append(ex_count)\n",
    "\n",
    "        X.append(feature_list)\n",
    "    return np.array(X)\n",
    "feature_selection_part2(X[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there may be killer asteroids headed for earth and nasa has decided to do something about it the space agency announced a new \"grand challenge\" on june 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet see also how it works nasa asteroid-captureresponses to the request for information which also seeks ideas for detecting and mitigating asteroid threats are due july 18the asteroid-retrieval mission designed to provide the first deep-space mission for astronauts flying on nasas space launch system rocket and orion space capsule under development has come under fire from lawmakers who would prefer that nasa return to the moona draft nasa authorization bill from the house space subcommittee which is currently in debate would cancel the mission and steer the agency toward other projects that bill will be discussed during a hearing wednesday june 19 at 10 am edtsee also how it works nasa asteroid-capture mission in picturesbut nasa officials defended the asteroid mission today and said they were confident theyd win congress support once they explained its benefits further\"i think that we really truly are going to be able to show the value of the mission\" nasa associate administrator lori garver said today \"to me this is something that what we do in this country — we debate how we spend the publics money this is the beginning of the debate\"garver also maintained that sending astronauts to an asteroid would not diminish nasas other science and exploration goals including another lunar landingsee also animation of proposed asteroid retrieval mission\"this initiative takes nothing from the other valuable work\" she said \"this is only a small piece of our overall strategy but it is an integral piece it takes nothing from the moon\"part of nasas plan to win support for the flight is to link it more closely with the larger goal of protecting earth from asteroid threatsif someday humanity discovers an asteroid headed for earth and manages to alter its course \"it will be one of the most important accomplishments in human history\" said tom kalil deputy director for technology and innovation at the white house office of science and technology policysee also wildest private deep-space mission ideas a countdownthe topic of asteroid threats is more timely than ever after a meteor exploded over the russian city of chelyabinsk on feb 15 — the same day that the football field-sized asteroid 2012 da14 passed within the moons orbit of earthimage courtesy of nasa \n"
     ]
    }
   ],
   "source": [
    "def feature_selection_part3(data):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "    paragraph = section.find_all(\"p\")\n",
    "    first = 0\n",
    "    last =\"\"\n",
    "    last_2=\"\"\n",
    "    for tag in paragraph:\n",
    "        if(not first):\n",
    "            feature_str += tag.text + \" \"\n",
    "            first = 1\n",
    "        else:\n",
    "            last_2=last\n",
    "            last = tag.text\n",
    "    if(last_2):feature_str += last_2 + \" \"\n",
    "    feature_str = re.sub(r'[.:\\',$()`]', '', feature_str.lower())\n",
    "    return feature_str\n",
    "\n",
    "print(feature_selection_part3(X[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random + GBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin vectorizer Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(ngram_range=(1, 2),\n",
       "                preprocessor=&lt;function feature_selection_part3 at 0x000001BDA847E980&gt;,\n",
       "                tokenizer=&lt;function tokenizer_stem_nostop at 0x000001BDA847E2A0&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 2),\n",
       "                preprocessor=&lt;function feature_selection_part3 at 0x000001BDA847E980&gt;,\n",
       "                tokenizer=&lt;function tokenizer_stem_nostop at 0x000001BDA847E2A0&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 2),\n",
       "                preprocessor=<function feature_selection_part3 at 0x000001BDA847E980>,\n",
       "                tokenizer=<function tokenizer_stem_nostop at 0x000001BDA847E2A0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=.2)\n",
    "print('Begin vectorizer Training...')\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection_part1,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(X)\n",
    "\n",
    "tfidf_type = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection_part3,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "tfidf_type.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Type Training...\n",
      "Begin Word Training...\n",
      "Begin Stats Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Begin Type Training...')\n",
    "type_model = SGDClassifier(loss='log_loss', max_iter=200)\n",
    "x_train_type = tfidf_type.transform(X_train)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "type_model.fit(x_train_type, y_train)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "word_model = RandomForestClassifier()\n",
    "x_train_word = tfidf.transform(X_train)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "word_model.fit(x_train_word, y_train)\n",
    "\n",
    "print('Begin Stats Training...')\n",
    "stats_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "x_train_stats = feature_selection_part2(X_train)\n",
    "stats_model.fit(x_train_stats, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Evaluation...\n",
      "Type Model\n",
      "Train score: 0.6221211501136162\n",
      "Valid score: 0.521331222692272\n",
      "Word Model\n",
      "Train score: 0.9999999959097893\n",
      "Valid score: 0.5413205778338172\n",
      "Stats Model\n",
      "Train score: 0.5943545781500547\n",
      "Valid score: 0.5632246327000101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Begin Evaluation...')\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)\n",
    "\n",
    "y_pred_type_train = type_model.predict_proba(x_train_type)[:,1]\n",
    "y_pred_word_train = word_model.predict_proba(x_train_word)[:,1]\n",
    "y_pred_stats_train = stats_model.predict_proba(x_train_stats)[:,1]\n",
    "\n",
    "y_pred_type_valid = type_model.predict_proba(tfidf_type.transform(X_valid))[:,1]\n",
    "y_pred_word_valid = word_model.predict_proba(tfidf.transform(X_valid))[:,1]\n",
    "y_pred_stats_valid = stats_model.predict_proba(feature_selection_part2(X_valid))[:,1]\n",
    "\n",
    "print(\"Type Model\")\n",
    "print(f'Train score: {roc_auc_score(y_train, y_pred_type_train)}')\n",
    "print(f'Valid score: {roc_auc_score(y_valid, y_pred_type_valid)}')\n",
    "\n",
    "print(\"Word Model\")\n",
    "print(f'Train score: {roc_auc_score(y_train, y_pred_word_train)}')\n",
    "print(f'Valid score: {roc_auc_score(y_valid, y_pred_word_valid)}')\n",
    "\n",
    "print(\"Stats Model\")\n",
    "print(f'Train score: {roc_auc_score(y_train, y_pred_stats_train)}')\n",
    "print(f'Valid score: {roc_auc_score(y_valid, y_pred_stats_valid)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Type Training...\n",
      "Type Model\n",
      "Train score: 0.9450321109362028\n",
      "Valid score: 0.543208698145595\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "print('Begin Type Training...')\n",
    "\n",
    "x_train_type = tfidf_type.transform(X_train)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "type_model.fit(x_train_type, y_train)\n",
    "y_pred_type_train = type_model.predict_proba(x_train_type)[:,1]\n",
    "y_pred_type_valid = type_model.predict_proba(tfidf_type.transform(X_valid))[:,1]\n",
    "print(\"Type Model\")\n",
    "print(f'Train score: {roc_auc_score(y_train, y_pred_type_train)}')\n",
    "print(f'Valid score: {roc_auc_score(y_valid, y_pred_type_valid)}')\n",
    "#0.5431"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find best para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find best para...\n",
      "Train score: 0.9907529577459627\n",
      "Valid score: 0.5756814738873636\n"
     ]
    }
   ],
   "source": [
    "train_score = []\n",
    "valid_score = []\n",
    "best_coef = []\n",
    "best_train = 0\n",
    "best_valid = 0\n",
    "coef_ = np.linspace(0,.5,21)\n",
    "print('Find best para...')\n",
    "for i in coef_:\n",
    "    for j in coef_:\n",
    "        train_score = roc_auc_score(y_train, y_pred_word_train*(i) + y_pred_stats_train*(1-i) + y_pred_type_train*(1-i-j))\n",
    "        valid_score = roc_auc_score(y_valid, y_pred_word_valid*(i) + y_pred_stats_valid*(1-i) + y_pred_type_valid*(1-i-j))\n",
    "        if valid_score > best_valid:\n",
    "            best_valid = valid_score\n",
    "            best_train = train_score\n",
    "            best_coef = [i,j,1-i-j]\n",
    "\n",
    "print(f'Train score: {best_train}')\n",
    "print(f'Valid score: {best_valid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(word_model, open(\"output/backup_word.pickle\", \"wb\"))\n",
    "pkl.dump(stats_model, open(\"output/backup_stats.pickle\", \"wb\"))\n",
    "pkl.dump(tfidf, open('output/tfidf_part1.pickle', \"wb\"))\n",
    "pkl.dump(tfidf_type, open('output/tfidf_first_paragraph.pickle', \"wb\"))\n",
    "pkl.dump(tfidf_type, open('output/backup_topic.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average weight error eurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('output'):\n",
    "    os.mkdir('output')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(train_auc)+1), train_auc, color='blue', label='Train auc')\n",
    "plt.plot(range(1, len(train_auc)+1), val_auc, color='red', label='Val auc')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('#Batches')\n",
    "plt.ylabel('Auc')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      " Id                                                          27643\n",
      "Page content    <html><head><div class=\"article-info\"><span cl...\n",
      "Name: 0, dtype: object\n",
      "(11847,)\n",
      "\n",
      "Id:\n",
      " 27643\n",
      "Content:\n",
      " <html><head><div class=\"article-info\"><span class=\"byline \"><a href=\"/author/sam-laird/\"><img alt=\"2016%2f09%2f15%2f63%2fhttpsd2mhye01h4nj2n.cloudfront.netmediazgkymde1lza2.9814b\" class=\"author_image\" src=\"http://i.amz.mshcdn.com/-qaMPB8aiQeIaoBhqlU0OLjA07A=/90x90/2016%2F09%2F15%2F63%2Fhttpsd2mhye01h4nj2n.cloudfront.netmediaZgkyMDE1LzA2.9814b.jpg\"/></a><span class=\"author_name\">By <a href=\"/author/sam-laird/\">Sam Laird</a></span><time datetime=\"Mon, 09 Sep 2013 19:47:02 +0000\">2013-09-09 19:47:02 UTC</time></span></div></head><body><h1 class=\"title\">Soccer Star Gets Twitter Death Threats After Tackling One Direction Member</h1><figure class=\"article-image\"></figure><article data-channel=\"entertainment\"><section class=\"article-content\"> <div class=\"shift-to-hero\"> <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"360\" src=\"https://www.youtube.com/embed/8L0lUoJ2BCY?enablejsapi=1&amp;\" width=\"640\"></iframe></p> <script src=\"http://a.amz.mshcdn.com/assets/lib/aab-7ce243b38b9cc2caec816aff811d3153.js\" type=\"text/javascript\"></script> </div> <p>Note to humanity: One Direction fandom ain't nothin' to mess with. </p> <p>A British <a href=\"http://mashable.com/category/soccer/\">soccer</a> star learned this hard way over the weekend when the pop band's fanboys and fangirls brought a ruckus to his <a href=\"http://mashable.com/category/twitter/\">Twitter</a> feed, tweeting him a fusillade of death threats following an incidental run-in Sunday during a charity soccer match.</p> <div class=\"see-also\"><p>See also: <a href=\"http://mashable.com/2013/07/30/gq-one-direction-death-threats/\">GQ Gets Twitter Death Threats for One Direction Magazine Covers</a></p></div> <p>Gabriel Agbonlahor is the all-time leading goal-scorer for English Premier League side Aston Villa. He participated in a charity match honoring a former player now battling leukemia. Also playing: Louis Tomlinson, one-fifth of the British-Irish boy band <a href=\"http://mashable.com/category/one-direction/\">One Direction</a>. </p> <p>Tomlinson was dribbling the ball about midway through Sunday's match when the pro striker Agbonlahor swooped in to knock it away from the tiny amateur. A clean play, to be sure, but there was some incidental contact as well, causing Tomlinson to collapse on the grass like a wounded deer. Then he limped off and promptly vomited into his hand, <a href=\"http://www.youtube.com/watch?v=19AADH8tcBM\" target=\"_blank\">as shown here</a>. </p> <p>Tomlinson and Agbonlahor shared a moment as he left the field and all seemed fine on the pitch. On Twitter, however, things were just getting started: One Direction's fan base exploded in a fit of mass rage, threatening to end Agbonlahor in all sorts of unpleasant ways. To whit, here are just a few examples, with some mildly NSFW language:</p> <blockquote class=\"twitter-tweet\"> <p>WHOEVER PUSHED LOUIS DOWN I WILL FIND YOU AND PUSH YOU OFF THE GODDAMN EMPIRE STATE BUILDING!</p> <p>— J a m R e y e s™ (@ItsJamReyes) <a href=\"https://twitter.com/ItsJamReyes/statuses/376714629201674241\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <blockquote class=\"twitter-tweet\"> <p>this is how Louis hurt his knee, I will find that guy, and kill him. <a href=\"https://t.co/3tbsGCT1bq\" target=\"_blank\">https://t.co/3tbsGCT1bq</a></p> <p>— PROUD OF LOUIS (@LatestOf1D) <a href=\"https://twitter.com/LatestOf1D/statuses/376715533560733696\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <blockquote class=\"twitter-tweet\"> <p><a href=\"https://twitter.com/gabby_10\" target=\"_blank\">@gabby_10</a> You obviously need Jesus. WE WILL FUCKING FIND YOU AND KILL YOU.</p> <p>— morgan (@NiallUrBasicUgh) <a href=\"https://twitter.com/NiallUrBasicUgh/statuses/376756036389982208\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <blockquote class=\"twitter-tweet\"> <p>fuck off you <a href=\"https://twitter.com/gabby_10\" target=\"_blank\">@gabby_10</a>, how could u touch my precious LOUIS? I WILL KILL U MUTHERFUCKER</p> <p>— Lucho García #51 (@Lois7x) <a href=\"https://twitter.com/Lois7x/statuses/376781833716760576\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <p>At least one person even took things a step further, threatening indiscriminate mass murder:</p> <blockquote class=\"twitter-tweet\"> <p>If i ever see louis cry bc he's hurt or sad i will kill the entire human race by myself</p> <p>— karol(arry) (@nuttelarry) <a href=\"https://twitter.com/nuttelarry/statuses/377046130682761216\" target=\"_blank\">September 9, 2013</a></p> </blockquote> <p>This is actually just the latest incident of rabid teenyboppers attacking <a href=\"http://mashable.com/sports/\">sports</a> stars online over perceived slights against their demigod idols of auto-tuned worship, however. </p> <p>If you're scoring at home, for example, you'll recall the time not so long ago that Justin Bieber fans <a href=\"http://mashable.com/2013/05/30/justin-bieber-fans-twitter-eric-dickerson/\">unleashed a digital fury</a> upon an NFL Hall of Famer following criticism of Bieber's alleged reckless driving. But the beef goes both ways, too. Hockey fans on Twitter <a href=\"http://mashable.com/2013/07/09/justin-bieber-stabley-cup/\">flew into a collective rage</a> in July after photos surfaced of Bieber posing in quintessentially Bieber-like fashion with the Stanley Cup. </p> <p><strong><div class=\"bonus-content\">BONUS: <a href=\"http://mashable.com/2013/08/04/one-direction-bullying-office-depot-school-supplies/\">One Direction and Directioners Fuse Online Powers to Battle Bullying</a> </div></strong></p> <section class=\"gallery\" data-display-mode=\"gallery\" data-id=\"11859\" data-slide-title-pos=\"\" data-slug=\"one-directions-anti-bullying-campaign-with-office-depot\"> <header> <h1>One Direction's Anti-Bullying Campaign With Office Depot</h1> </header> <ol class=\"slides\"> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced00048d\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/Un3ZihG1VmZWnvs-VprxzwDgFOI=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2F0Fr6eIUoB6I.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/0Fr6eIUoB6I\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">1. 1D + OD Together Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced00048e\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/oOrgEGOY7kCyASO2sJEuiJmcH4Y=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2F2RuwAne1GRk.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/2RuwAne1GRk\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">2. Harry Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced00048f\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/slMRLhuwNWBmi2WUWOsoChIkbT4=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2FIVQeItw5NHQ.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/IVQeItw5NHQ\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">3. Zayn Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced000490\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/eVdXXGFMQcb8Dd0CZ95pp3etjs4=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2FL6Wdbq6frxY.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/L6Wdbq6frxY\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">4. Niall Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced000491\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/-IEKxbwTTehrbTiUHsXA4gv3L4c=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2F6mvTafur08w.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/6mvTafur08w\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">5. Louis Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced000492\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/U0yZuGwxU2fTisly0txtcIBENkg=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2FUlg966fRbJQ.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/Ulg966fRbJQ\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">6. Liam Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> </ol> </section> <p><em>Image: Ian Gavan/Getty Images for Sony Pictures</em></p> <script>      window._msla=window.loadScriptAsync||function(src,id){if(document.getElementById(id))return;var js=document.createElement('script');js.id=id;js.src=src;document.getElementsByTagName('script')[0].parentNode.insertBefore(js,fjs);}; _msla(\"//platform.twitter.com/widgets.js\",\"twitter_jssdk\");</script> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/entertainment/\">Entertainment</a>, <a href=\"/category/music/\">Music</a>, <a href=\"/category/one-direction/\">One Direction</a>, <a href=\"/category/soccer/\">soccer</a>, <a href=\"/category/sports/\">Sports</a> </footer></body></html>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/test.csv')\n",
    "print(\"Content:\\n\",df.loc[0])\n",
    "\n",
    "Id = df.loc[:, 'Id'].to_numpy()\n",
    "X_test = df.loc[:, 'Page content'].to_numpy()\n",
    "print(X_test.shape)\n",
    "print(\"\\nId:\\n\", Id[0])\n",
    "print(\"Content:\\n\", X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 38\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x_test_type \u001b[39m=\u001b[39m tfidf_type\u001b[39m.\u001b[39mtransform(X_test)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x_test_word \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39;49mtransform(X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x_test_stats \u001b[39m=\u001b[39m feature_selection_part2(X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_pred_type_test \u001b[39m=\u001b[39m type_model\u001b[39m.\u001b[39mpredict_proba(x_test_type)[:,\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2163\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2146\u001b[0m \u001b[39m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2147\u001b[0m \n\u001b[0;32m   2148\u001b[0m \u001b[39mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2159\u001b[0m \u001b[39m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2161\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m, msg\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2163\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtransform(raw_documents)\n\u001b[0;32m   2164\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mtransform(X, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1434\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1433\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1434\u001b[0m _, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, fixed_vocab\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1436\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 38\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m pos_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m paragraph:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     pos_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer_stem_pos(tag\u001b[39m.\u001b[39;49mtext))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokenizer_stem_pos(tag\u001b[39m.\u001b[39mtext):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         feature_str \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m word \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenizer_stem_pos\u001b[39m(text):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     stemmer \u001b[39m=\u001b[39m LancasterStemmer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [stemmer\u001b[39m.\u001b[39;49mstem(w) \u001b[39mfor\u001b[39;49;00m w \u001b[39min\u001b[39;49;00m re\u001b[39m.\u001b[39;49msplit(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms+\u001b[39;49m\u001b[39m'\u001b[39;49m, text\u001b[39m.\u001b[39;49mstrip()) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39mif\u001b[39;49;00m w \u001b[39min\u001b[39;49;00m pos_words \u001b[39mand\u001b[39;49;00m re\u001b[39m.\u001b[39;49mmatch(\u001b[39m'\u001b[39;49m\u001b[39m[a-zA-Z]+\u001b[39;49m\u001b[39m'\u001b[39;49m, w)]\n",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenizer_stem_pos\u001b[39m(text):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     stemmer \u001b[39m=\u001b[39m LancasterStemmer()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [stemmer\u001b[39m.\u001b[39mstem(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m re\u001b[39m.\u001b[39msplit(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+\u001b[39m\u001b[39m'\u001b[39m, text\u001b[39m.\u001b[39mstrip()) \\\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             \u001b[39mif\u001b[39;00m w \u001b[39min\u001b[39;00m pos_words \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39mmatch(\u001b[39m'\u001b[39m\u001b[39m[a-zA-Z]+\u001b[39m\u001b[39m'\u001b[39m, w)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_test_type = tfidf_type.transform(X_test)\n",
    "x_test_word = tfidf.transform(X_test)\n",
    "x_test_stats = feature_selection_part2(X_test)\n",
    "\n",
    "y_pred_type_test = type_model.predict_proba(x_test_type)[:,1]\n",
    "y_pred_word_test = word_model.predict_proba(x_test_word)[:,1]\n",
    "y_pred_stats_test = stats_model.predict_proba(x_test_stats)[:,1]\n",
    "y_pred = np.around(y_pred_word_test*(i) + y_pred_stats_test*(j) + y_pred_type_test*(1-i-j), decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {'Id': Id, 'Popularity': y_pred}\n",
    "output_dataframe = pd.DataFrame(output_data)\n",
    "print(output_dataframe)\n",
    "\n",
    "output_dataframe.to_csv(\"./datasets/y_pred.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
