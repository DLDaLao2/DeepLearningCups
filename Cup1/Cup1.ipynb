{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from scipy.sparse import hstack\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/train.csv')\n",
    "X = df.loc[:, 'Page content'].to_numpy()\n",
    "y = df.loc[:,'Popularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'positive-words.txt')\n",
    "NEGATIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'negative-words.txt')\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "\n",
    "for line in open(POSITIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    pos_words.append(word)\n",
    "\n",
    "for line in open(NEGATIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    neg_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_neg(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in neg_words and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def tokenizer_stem_pos(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in pos_words and re.match('[a-zA-Z]+', w)]\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of mem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "batch_size = 1024\n",
    "iters = int((27643+batch_size-1)/(batch_size*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = get_stream(path='./datasets/train.csv', size=batch_size)\n",
    "idx = 0\n",
    "X_train, y_train = None, None\n",
    "batch_X, batch_y = None, None\n",
    "for z in range(iters):\n",
    "    batch = next(stream)\n",
    "    if(idx==0):\n",
    "        X_train= batch['Page content']\n",
    "        y_train = batch['Popularity']\n",
    "        idx+=1\n",
    "    else:\n",
    "        X_train = np.concatenate((X_train, batch['Page content']))\n",
    "        y_train = np.concatenate((y_train, batch['Popularity']))\n",
    "    batch = next(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasas grand challenge stop asteroids from destroying earth there may be killer asteroids headed for earth, and nasa has decided to do something about it the space agency announced a new \"grand challenge\" on june 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet see also how it works nasa asteroid-captureresponses to the request for information, which also seeks ideas for detecting and mitigating asteroid threats, are due july 18the asteroid-retrieval mission, designed to provide the first deep-space mission for astronauts flying on nasas space launch system rocket and orion space capsule under development, has come under fire from lawmakers who would prefer that nasa return to the moona draft nasa authorization bill from the house space subcommittee, which is currently in debate, would cancel the mission and steer the agency toward other projects that bill will be discussed during a hearing wednesday, june 19 at 10 am edtsee also how it works nasa asteroid-capture mission in picturesbut nasa officials defended the asteroid mission today and said they were confident theyd win congress support once they explained its benefits further\"i think that we really, truly are going to be able to show the value of the mission,\" nasa associate administrator lori garver said today \"to me, this is something that what we do in this country — we debate how we spend the publics money this is the beginning of the debate\"garver also maintained that sending astronauts to an asteroid would not diminish nasas other science and exploration goals, including another lunar landingsee also animation of proposed asteroid retrieval mission\"this initiative takes nothing from the other valuable work,\" she said \"this is only a small piece of our overall strategy, but it is an integral piece it takes nothing from the moon\"part of nasas plan to win support for the flight is to link it more closely with the larger goal of protecting earth from asteroid threatsif, someday, humanity discovers an asteroid headed for earth and manages to alter its course, \"it will be one of the most important accomplishments in human history,\" said tom kalil, deputy director for technology and innovation at the white house office of science and technology policysee also wildest private deep-space mission ideas a countdownthe topic of asteroid threats is more timely than ever, after a meteor exploded over the russian city of chelyabinsk on feb 15 — the same day that the football field-sized asteroid 2012 da14 passed within the moons orbit of earthimage courtesy of nasa \n"
     ]
    }
   ],
   "source": [
    "def feature_selection_part1(data):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    feature_str += soup.find(\"h1\", {\"class\": \"title\"}).text + \" \"\n",
    "\n",
    "    # # Channel\n",
    "    # feature_str += soup.find(\"article\").get(\"data-channel\") + \" \"\n",
    "\n",
    "    # # Related Topics\n",
    "    # feature_str += soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").replace(\",\", \"\")\n",
    "    \n",
    "    section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "    paragraph = section.find_all(\"p\")\n",
    "    first = 0\n",
    "    last =\"\"\n",
    "    last_2=\"\"\n",
    "    for tag in paragraph:\n",
    "        if(not first):\n",
    "            feature_str += tag.text + \" \"\n",
    "            first = 1\n",
    "        else:\n",
    "            last_2=last\n",
    "            last = tag.text\n",
    "    if(last_2):feature_str += last_2 + \" \"\n",
    "    feature_str = re.sub(r'[.:\\']', '', feature_str.lower())\n",
    "    \n",
    "    return feature_str\n",
    "print(feature_selection_part1(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'avg_channel' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X25sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m         X\u001b[39m.\u001b[39mappend(feature_list)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X25sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(X)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X25sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m feature_selection_part2(X[\u001b[39m0\u001b[39;49m:\u001b[39m10\u001b[39;49m],\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X25sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mfor\u001b[39;00m html \u001b[39min\u001b[39;00m data:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X25sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mfor\u001b[39;00m c,s \u001b[39min\u001b[39;00m channel_score\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X25sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         avg_channel\u001b[39m+\u001b[39m\u001b[39m=\u001b[39ms\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X25sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m         total_channel\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mchannel_num[c]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X25sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     avg_channel \u001b[39m=\u001b[39m avg_channel\u001b[39m/\u001b[39mtotal_channel\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'avg_channel' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "author_score =dict()\n",
    "author_num = dict()\n",
    "channel_score = dict()\n",
    "channel_num = dict()\n",
    "topic_score = dict()\n",
    "topic_num = dict()\n",
    "avg_author = 0\n",
    "avg_channel = 0\n",
    "avg_topic = 0\n",
    "\n",
    "def feature_selection_part2(data, istraining):\n",
    "    X = []\n",
    "    idx=0\n",
    "    global author_score\n",
    "    global author_num \n",
    "    global channel_score \n",
    "    global channel_num \n",
    "    global topic_score \n",
    "    global topic_num \n",
    "    global avg_author \n",
    "    global avg_channel \n",
    "    global avg_topic \n",
    "    \n",
    "    if(istraining):\n",
    "        for html in data:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            channel = soup.find(\"article\").get(\"data-channel\")\n",
    "            if channel in channel_score:\n",
    "                channel_score[channel] += 1 if int(y[idx])==1 else  0\n",
    "                channel_num[channel] += 1\n",
    "            else : \n",
    "                channel_score[channel] = 0\n",
    "                channel_score[channel] += 1 if int(y[idx])==1 else  0\n",
    "                channel_num[channel] = 1\n",
    "            \n",
    "            topics = soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\")\n",
    "            for topic in topics:\n",
    "                if topic in topic_score:\n",
    "                    topic_score[topic] += 1 if int(y[idx])==1 else  0\n",
    "                    topic_num[topic] += 1\n",
    "                else : \n",
    "                    topic_score[topic] = 0\n",
    "                    topic_score[topic] += 1 if int(y[idx])==1 else  0\n",
    "                    topic_num[topic] = 1\n",
    "\n",
    "\n",
    "            # Author\n",
    "            author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "            if soup.head.find(\"span\") == None:\n",
    "                continue\n",
    "            else:\n",
    "                author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "                if author in author_score:\n",
    "                    author_score[author] += 1 if int(y[idx])==1 else  0\n",
    "                    author_num[author] += 1\n",
    "                else : \n",
    "                    author_score[author] = 0\n",
    "                    author_num[author] = 1\n",
    "                    author_score[author] += 1 if int(y[idx])==1 else  0\n",
    "            idx+=1\n",
    "        total_channel = 0\n",
    "        total_topic = 0\n",
    "        total_author = 0\n",
    "        for html in data:\n",
    "            for c,s in channel_score.items():\n",
    "                avg_channel+=s\n",
    "                total_channel+=channel_num[c]\n",
    "            avg_channel = avg_channel/total_channel\n",
    "\n",
    "            for c,s in topic_score.items():\n",
    "                avg_topic+=s\n",
    "                total_topic+=topic_num[c]\n",
    "            avg_topic = avg_topic/total_topic\n",
    "\n",
    "            for c,s in author_score.items():\n",
    "                avg_author+=s\n",
    "                total_author+=author_num[c]\n",
    "            avg_author = avg_author/total_author\n",
    "\n",
    "    for html in data:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        feature_list = []\n",
    "        # Author score\n",
    "        author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "        if soup.head.find(\"span\") == None:\n",
    "            feature_list.append(avg_author)\n",
    "        else:\n",
    "            author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "            if author in author_score:\n",
    "                feature_list.append(author_score[author]/author_num[author])\n",
    "            else:\n",
    "                feature_list.append(avg_author)\n",
    "        # Channel score\n",
    "        channel = soup.find(\"article\").get(\"data-channel\")\n",
    "        if channel in channel_score:\n",
    "            feature_list.append(channel_score[channel]/channel_num[channel])\n",
    "        else:\n",
    "            feature_list.append(avg_channel)\n",
    "        # Topic score\n",
    "        topics = soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\")\n",
    "        total_score = 0\n",
    "        order = 0 \n",
    "        order_denominator = 0\n",
    "        for i in range(len(topics)):\n",
    "            order_denominator += math.exp(-0.5*i)\n",
    "\n",
    "        for topic in topics:\n",
    "            order_coef = math.exp(-0.5*order)/order_denominator\n",
    "            if topic in topic_score:\n",
    "                total_score+=(topic_score[topic]/topic_num[topic])*order_coef\n",
    "            else:\n",
    "                total_score+=avg_topic*order_coef\n",
    "            order+=1\n",
    "        \n",
    "        feature_list.append(total_score) \n",
    "\n",
    "        # word count of title\n",
    "        feature_list.append(len(soup.find(\"h1\", {\"class\": \"title\"}).text))\n",
    "        # average word length and unique word rate\n",
    "        words = re.findall(r'\\w+', soup.get_text().lower())\n",
    "        if words:\n",
    "            total_words = len(words)\n",
    "            unique_words = set(words)\n",
    "            unique_word_count = len(unique_words)\n",
    "            total_length = sum(len(word) for word in words)\n",
    "            unique_rate = unique_word_count / total_words\n",
    "            average_length = total_length / len(words)\n",
    "            feature_list.append(unique_rate)\n",
    "            feature_list.append(average_length)\n",
    "        \n",
    "        # Time\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            month = int(re.search(r'(\\d+-\\d+-\\d+)', soup.time.text).group(1).split(\"-\")[1])\n",
    "            feature_list.append(month)\n",
    "            hour = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[0])\n",
    "            minute = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[1])\n",
    "            feature_list.append(hour * 60 + minute)\n",
    "\n",
    "        # Weekend\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            feature_list.append(1 if soup.time.get(\"datetime\")[:3] in [\"Sat\", \"Sun\"] else 0)\n",
    "\n",
    "        # Word Count\n",
    "        text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "        c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "        text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "        c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "        total = c_div + c_p\n",
    "        feature_list.append(len(list(total.elements())))\n",
    "\n",
    "        section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "\n",
    "        # Video + Image count\n",
    "        img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "        video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "        media_count = img_count + video_count\n",
    "        feature_list.append(media_count)\n",
    "\n",
    "        # Appealing count\n",
    "        link_count = len(section.find_all(\"a\"))\n",
    "        strong_count = len(section.find_all(\"strong\"))\n",
    "        appealing_count = link_count + strong_count\n",
    "        feature_list.append(appealing_count)\n",
    "\n",
    "        # POS & NEG count\n",
    "        paragraph = section.find_all(\"p\")\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        q_count = 0\n",
    "        ex_count = 0\n",
    "        for tag in paragraph:\n",
    "            pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "            neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "            if tag.text.find(\"?\") != -1:\n",
    "                q_count += tag.text.find(\"?\")\n",
    "            if tag.text.find(\"!\") != -1:\n",
    "                ex_count += tag.text.find(\"!\")\n",
    "        feature_list.append(pos_count)    \n",
    "        feature_list.append(neg_count)\n",
    "        feature_list.append(q_count)\n",
    "        feature_list.append(ex_count)\n",
    "\n",
    "        X.append(feature_list)\n",
    "    return np.array(X)\n",
    "feature_selection_part2(X[0:10],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def feature_selection_part3(data):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "    \n",
    "    feature_str = re.sub(r'[.:\\',$()`]', '', feature_str.lower())\n",
    "    return feature_str\n",
    "\n",
    "print(feature_selection_part3(X[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random + GBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin vectorizer Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 17\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBegin vectorizer Training...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                         preprocessor\u001b[39m=\u001b[39mfeature_selection_part1,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                         tokenizer\u001b[39m=\u001b[39mtokenizer_stem_nostop)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tfidf\u001b[39m.\u001b[39;49mfit(X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# tfidf_type = TfidfVectorizer(ngram_range=(1,2),\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#                         preprocessor=feature_selection_part3,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#                         tokenizer=tokenizer_stem_nostop)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# tfidf_type.fit(X)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2109\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2104\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2105\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2106\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2107\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2108\u001b[0m )\n\u001b[1;32m-> 2109\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2111\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1277\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "\u001b[1;32mc:\\Users\\johnson\\Documents\\CODE\\DeepLearningCups\\Cup1\\Cup1.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeature_selection_part1\u001b[39m(data):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     feature_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(data, \u001b[39m'\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Title\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/johnson/Documents/CODE/DeepLearningCups/Cup1/Cup1.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     feature_str \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mh1\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m})\u001b[39m.\u001b[39mtext \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\bs4\\__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[0;32m    334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed()\n\u001b[0;32m    336\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\bs4\\__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 478\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mfeed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[0;32m    479\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    378\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[0;32m    379\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     parser\u001b[39m.\u001b[39;49mfeed(markup)\n\u001b[0;32m    381\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    382\u001b[0m     \u001b[39m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[0;32m    383\u001b[0m     \u001b[39m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[39m# when there's an error in the doctype declaration.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[39mraise\u001b[39;00m ParserRejectedMarkup(e)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\html\\parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[1;32m--> 110\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoahead(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\html\\parser.py:170\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m startswith(\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m, i):\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m starttagopen\u001b[39m.\u001b[39mmatch(rawdata, i): \u001b[39m# < + letter\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_starttag(i)\n\u001b[0;32m    171\u001b[0m     \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[0;32m    172\u001b[0m         k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_endtag(i)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\html\\parser.py:337\u001b[0m, in \u001b[0;36mHTMLParser.parse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_startendtag(tag, attrs)\n\u001b[0;32m    336\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 337\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_starttag(tag, attrs)\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m tag \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCDATA_CONTENT_ELEMENTS:\n\u001b[0;32m    339\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_cdata_mode(tag)\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:141\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_starttag\u001b[1;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[0;32m    136\u001b[0m sourceline, sourcepos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetpos()\n\u001b[0;32m    137\u001b[0m tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\u001b[39m.\u001b[39mhandle_starttag(\n\u001b[0;32m    138\u001b[0m     name, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, attr_dict, sourceline\u001b[39m=\u001b[39msourceline,\n\u001b[0;32m    139\u001b[0m     sourcepos\u001b[39m=\u001b[39msourcepos\n\u001b[0;32m    140\u001b[0m )\n\u001b[1;32m--> 141\u001b[0m \u001b[39mif\u001b[39;00m tag \u001b[39mand\u001b[39;00m tag\u001b[39m.\u001b[39;49mis_empty_element \u001b[39mand\u001b[39;00m handle_empty_element:\n\u001b[0;32m    142\u001b[0m     \u001b[39m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[39m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[39m# handle_startendtag, but only if the original markup looked like\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[39m# <tag/>.)\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[39m# So we need to call handle_endtag() ourselves. Since we\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[39m# know the start event is identical to the end event, we\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     \u001b[39m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[39m# events for tags of this name.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_endtag(name, check_already_closed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    153\u001b[0m     \u001b[39m# But we might encounter an explicit closing tag for this tag\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[39m# later on. If so, we want to ignore it.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\bs4\\element.py:1371\u001b[0m, in \u001b[0;36mTag.is_empty_element\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1368\u001b[0m         \u001b[39msetattr\u001b[39m(clone, attr, \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr))\n\u001b[0;32m   1369\u001b[0m     \u001b[39mreturn\u001b[39;00m clone\n\u001b[1;32m-> 1371\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m   1372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_empty_element\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1373\u001b[0m     \u001b[39m\"\"\"Is this tag an empty-element tag? (aka a self-closing tag)\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \n\u001b[0;32m   1375\u001b[0m \u001b[39m    A tag that has contents is never an empty-element tag.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[39m    then any tag with no contents is an empty-element tag.\u001b[39;00m\n\u001b[0;32m   1385\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1386\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontents) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_be_empty_element\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=.2)\n",
    "print('Begin vectorizer Training...')\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection_part1,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(X)\n",
    "\n",
    "# tfidf_type = TfidfVectorizer(ngram_range=(1,2),\n",
    "#                         preprocessor=feature_selection_part3,\n",
    "#                         tokenizer=tokenizer_stem_nostop)\n",
    "# tfidf_type.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Begin Type Training...')\n",
    "# type_model = SGDClassifier(loss='log_loss', max_iter=200)\n",
    "# x_train_type = tfidf_type.transform(X_train)\n",
    "# y_train = LabelEncoder().fit_transform(Y_train)\n",
    "# type_model.fit(x_train_type, y_train)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "word_model = RandomForestClassifier(n_estimator=100, max_features=0.4, max_depth=5, min_samples_leaf=5)\n",
    "x_train_word = tfidf.transform(X_train)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "word_model.fit(x_train_word, y_train)\n",
    "\n",
    "print('Begin Stats Training...')\n",
    "stats_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "x_train_stats = feature_selection_part2(X_train, True)\n",
    "stats_model.fit(x_train_stats, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Begin Evaluation...')\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)\n",
    "\n",
    "# y_pred_type_train = type_model.predict_proba(x_train_type)[:,1]\n",
    "# y_pred_type_valid = type_model.predict_proba(tfidf_type.transform(X_valid))[:,1]\n",
    "\n",
    "y_pred_word_train = word_model.predict_proba(x_train_word)[:,1]\n",
    "y_pred_word_valid = word_model.predict_proba(tfidf.transform(X_valid))[:,1]\n",
    "\n",
    "y_pred_stats_train = stats_model.predict_proba(x_train_stats)[:,1]\n",
    "y_pred_stats_valid = stats_model.predict_proba(feature_selection_part2(X_valid, False))[:,1]\n",
    "\n",
    "# print(\"Type Model\")\n",
    "# print(f'Train score: {roc_auc_score(y_train, y_pred_type_train)}')\n",
    "# print(f'Valid score: {roc_auc_score(y_valid, y_pred_type_valid)}')\n",
    "\n",
    "print(\"Word Model\")\n",
    "print(f'Train score: {roc_auc_score(y_train, y_pred_word_train)}')\n",
    "print(f'Valid score: {roc_auc_score(y_valid, y_pred_word_valid)}')\n",
    "\n",
    "print(\"Stats Model\")\n",
    "print(f'Train score: {roc_auc_score(y_train, y_pred_stats_train)}')\n",
    "print(f'Valid score: {roc_auc_score(y_valid, y_pred_stats_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find best para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "valid_score = []\n",
    "best_coef = []\n",
    "best_train = 0\n",
    "best_valid = 0\n",
    "coef_ = np.linspace(0,.5,21)\n",
    "print('Find best para...')\n",
    "for i in coef_:\n",
    "    # for j in coef_:\n",
    "    # train_score = roc_auc_score(y_train, y_pred_word_train*(i) + y_pred_stats_train*(1-i) + y_pred_type_train*(1-i-j))\n",
    "    # valid_score = roc_auc_score(y_valid, y_pred_word_valid*(i) + y_pred_stats_valid*(1-i) + y_pred_type_valid*(1-i-j))\n",
    "    train_score = roc_auc_score(y_train, y_pred_word_train*(i) + y_pred_stats_train*(1-i))\n",
    "    valid_score = roc_auc_score(y_valid, y_pred_word_valid*(i) + y_pred_stats_valid*(1-i))\n",
    "    if valid_score > best_valid:\n",
    "        best_valid = valid_score\n",
    "        best_train = train_score\n",
    "        best_coef = [i,1-i]\n",
    "        # best_coef = [i,j,1-i-j]\n",
    "\n",
    "print(f'Train score: {best_train}')\n",
    "print(f'Valid score: {best_valid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(word_model, open(\"output/backup_word.pickle\", \"wb\"))\n",
    "pkl.dump(stats_model, open(\"output/backup_stats.pickle\", \"wb\"))\n",
    "# pkl.dump(type_model, open('output/backup_topic.pickle', \"wb\"))\n",
    "pkl.dump(tfidf, open('output/tfidf_part1.pickle', \"wb\"))\n",
    "# pkl.dump(tfidf_type, open('output/tfidf_first_paragraph.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/test.csv')\n",
    "print(\"Content:\\n\",df.loc[0])\n",
    "\n",
    "Id = df.loc[:, 'Id'].to_numpy()\n",
    "X_test = df.loc[:, 'Page content'].to_numpy()\n",
    "print(X_test.shape)\n",
    "print(\"\\nId:\\n\", Id[0])\n",
    "print(\"Content:\\n\", X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_type = tfidf_type.transform(X_test)\n",
    "x_test_word = tfidf.transform(X_test)\n",
    "x_test_stats = feature_selection_part2(X_test)\n",
    "\n",
    "# y_pred_type_test = type_model.predict_proba(x_test_type)[:,1]\n",
    "y_pred_word_test = word_model.predict_proba(x_test_word)[:,1]\n",
    "y_pred_stats_test = stats_model.predict_proba(x_test_stats)[:,1]\n",
    "y_pred = np.around(y_pred_word_test*(best_coef[0]) + y_pred_stats_test*(best_coef[1]), decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {'Id': Id, 'Popularity': y_pred}\n",
    "output_dataframe = pd.DataFrame(output_data)\n",
    "print(output_dataframe)\n",
    "\n",
    "output_dataframe.to_csv(\"./datasets/y_pred.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
