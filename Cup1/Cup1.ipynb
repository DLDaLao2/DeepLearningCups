{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from scipy.sparse import hstack\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/train.csv')\n",
    "X = df.loc[:, 'Page content'].to_numpy()\n",
    "y = df.loc[:,'Popularity'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'positive-words.txt')\n",
    "NEGATIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'negative-words.txt')\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "\n",
    "for line in open(POSITIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    pos_words.append(word)\n",
    "\n",
    "for line in open(NEGATIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    neg_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_neg(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in neg_words and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def tokenizer_stem_pos(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in pos_words and re.match('[a-zA-Z]+', w)]\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of mem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "batch_size = 1024\n",
    "iters = int((27643+batch_size-1)/(batch_size*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = get_stream(path='./datasets/train.csv', size=batch_size)\n",
    "idx = 0\n",
    "X_train, y_train = None, None\n",
    "batch_X, batch_y = None, None\n",
    "for z in range(iters):\n",
    "    batch = next(stream)\n",
    "    if(idx==0):\n",
    "        X_train= batch['Page content']\n",
    "        y_train = batch['Popularity']\n",
    "        idx+=1\n",
    "    else:\n",
    "        X_train = np.concatenate((X_train, batch['Page content']))\n",
    "        y_train = np.concatenate((y_train, batch['Popularity']))\n",
    "    batch = next(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasas grand challenge stop asteroids from destroying earth there may be killer asteroids headed for earth, and nasa has decided to do something about it the space agency announced a new \"grand challenge\" on june 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet see also how it works nasa asteroid-captureresponses to the request for information, which also seeks ideas for detecting and mitigating asteroid threats, are due july 18the asteroid-retrieval mission, designed to provide the first deep-space mission for astronauts flying on nasas space launch system rocket and orion space capsule under development, has come under fire from lawmakers who would prefer that nasa return to the moona draft nasa authorization bill from the house space subcommittee, which is currently in debate, would cancel the mission and steer the agency toward other projects that bill will be discussed during a hearing wednesday, june 19 at 10 am edtsee also how it works nasa asteroid-capture mission in picturesbut nasa officials defended the asteroid mission today and said they were confident theyd win congress support once they explained its benefits further\"i think that we really, truly are going to be able to show the value of the mission,\" nasa associate administrator lori garver said today \"to me, this is something that what we do in this country — we debate how we spend the publics money this is the beginning of the debate\"garver also maintained that sending astronauts to an asteroid would not diminish nasas other science and exploration goals, including another lunar landingsee also animation of proposed asteroid retrieval mission\"this initiative takes nothing from the other valuable work,\" she said \"this is only a small piece of our overall strategy, but it is an integral piece it takes nothing from the moon\"part of nasas plan to win support for the flight is to link it more closely with the larger goal of protecting earth from asteroid threatsif, someday, humanity discovers an asteroid headed for earth and manages to alter its course, \"it will be one of the most important accomplishments in human history,\" said tom kalil, deputy director for technology and innovation at the white house office of science and technology policysee also wildest private deep-space mission ideas a countdownthe topic of asteroid threats is more timely than ever, after a meteor exploded over the russian city of chelyabinsk on feb 15 — the same day that the football field-sized asteroid 2012 da14 passed within the moons orbit of earthimage courtesy of nasa \n"
     ]
    }
   ],
   "source": [
    "def feature_selection_part1(data):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    feature_str += soup.find(\"h1\", {\"class\": \"title\"}).text + \" \"\n",
    "\n",
    "    # # Channel\n",
    "    # feature_str += soup.find(\"article\").get(\"data-channel\") + \" \"\n",
    "\n",
    "    # # Related Topics\n",
    "    # feature_str += soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").replace(\",\", \"\")\n",
    "    \n",
    "    section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "    paragraph = section.find_all(\"p\")\n",
    "    first = 0\n",
    "    last =\"\"\n",
    "    last_2=\"\"\n",
    "    for tag in paragraph:\n",
    "        if(not first):\n",
    "            feature_str += tag.text + \" \"\n",
    "            first = 1\n",
    "        else:\n",
    "            last_2=last\n",
    "            last = tag.text\n",
    "    if(last_2):feature_str += last_2 + \" \"\n",
    "    feature_str = re.sub(r'[.:\\']', '', feature_str.lower())\n",
    "    \n",
    "    return feature_str\n",
    "print(feature_selection_part1(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "author_score =dict()\n",
    "author_num = dict()\n",
    "channel_score = dict()\n",
    "channel_num = dict()\n",
    "topic_score = dict()\n",
    "topic_num = dict()\n",
    "avg_author = 0\n",
    "avg_channel = 0\n",
    "avg_topic = 0\n",
    "\n",
    "def feature_selection_part2(data, istraining):\n",
    "    X = []\n",
    "    idx=0\n",
    "    global author_score\n",
    "    global author_num \n",
    "    global channel_score \n",
    "    global channel_num \n",
    "    global topic_score \n",
    "    global topic_num \n",
    "    global avg_author \n",
    "    global avg_channel \n",
    "    global avg_topic \n",
    "    \n",
    "    if(istraining):\n",
    "        for html in data:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            channel = soup.find(\"article\").get(\"data-channel\")\n",
    "            if channel in channel_score:\n",
    "                channel_score[channel] += 1 if int(y[idx])==1 else  0\n",
    "                channel_num[channel] += 1\n",
    "            else : \n",
    "                channel_score[channel] = 0\n",
    "                channel_score[channel] += 1 if int(y[idx])==1 else  0\n",
    "                channel_num[channel] = 1\n",
    "            \n",
    "            topics = soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\")\n",
    "            for topic in topics:\n",
    "                if topic in topic_score:\n",
    "                    topic_score[topic] += 1 if int(y[idx])==1 else  0\n",
    "                    topic_num[topic] += 1\n",
    "                else : \n",
    "                    topic_score[topic] = 0\n",
    "                    topic_score[topic] += 1 if int(y[idx])==1 else  0\n",
    "                    topic_num[topic] = 1\n",
    "\n",
    "\n",
    "            # Author\n",
    "            author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "            if soup.head.find(\"span\") == None:\n",
    "                continue\n",
    "            else:\n",
    "                author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "                if author in author_score:\n",
    "                    author_score[author] += 1 if int(y[idx])==1 else  0\n",
    "                    author_num[author] += 1\n",
    "                else : \n",
    "                    author_score[author] = 0\n",
    "                    author_num[author] = 1\n",
    "                    author_score[author] += 1 if int(y[idx])==1 else  0\n",
    "            idx+=1\n",
    "        total_channel = 0\n",
    "        total_topic = 0\n",
    "        total_author = 0\n",
    "        for html in data:\n",
    "            for c,s in channel_score.items():\n",
    "                avg_channel+=s\n",
    "                total_channel+=channel_num[c]\n",
    "            avg_channel = avg_channel/total_channel\n",
    "\n",
    "            for c,s in topic_score.items():\n",
    "                avg_topic+=s\n",
    "                total_topic+=topic_num[c]\n",
    "            avg_topic = avg_topic/total_topic\n",
    "\n",
    "            for c,s in author_score.items():\n",
    "                avg_author+=s\n",
    "                total_author+=author_num[c]\n",
    "            avg_author = avg_author/total_author\n",
    "\n",
    "    for html in data:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        feature_list = []\n",
    "        # Author score\n",
    "        author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "        if soup.head.find(\"span\") == None:\n",
    "            feature_list.append(avg_author)\n",
    "        else:\n",
    "            author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "            if author in author_score:\n",
    "                feature_list.append(author_score[author]/author_num[author])\n",
    "            else:\n",
    "                feature_list.append(avg_author)\n",
    "        # Channel score\n",
    "        channel = soup.find(\"article\").get(\"data-channel\")\n",
    "        if channel in channel_score:\n",
    "            feature_list.append(channel_score[channel]/channel_num[channel])\n",
    "        else:\n",
    "            feature_list.append(avg_channel)\n",
    "        # Topic score\n",
    "        topics = soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\")\n",
    "        total_score = 0\n",
    "        order = 0 \n",
    "        order_denominator = 0\n",
    "        for i in range(len(topics)):\n",
    "            order_denominator += math.exp(-0.5*i)\n",
    "\n",
    "        for topic in topics:\n",
    "            order_coef = math.exp(-0.5*order)/order_denominator\n",
    "            if topic in topic_score:\n",
    "                total_score+=(topic_score[topic]/topic_num[topic])*order_coef\n",
    "            else:\n",
    "                total_score+=avg_topic*order_coef\n",
    "            order+=1\n",
    "        \n",
    "        feature_list.append(total_score) \n",
    "\n",
    "        # word count of title\n",
    "        feature_list.append(len(soup.find(\"h1\", {\"class\": \"title\"}).text))\n",
    "        # average word length and unique word rate\n",
    "        words = re.findall(r'\\w+', soup.get_text().lower())\n",
    "        if words:\n",
    "            total_words = len(words)\n",
    "            unique_words = set(words)\n",
    "            unique_word_count = len(unique_words)\n",
    "            total_length = sum(len(word) for word in words)\n",
    "            unique_rate = unique_word_count / total_words\n",
    "            average_length = total_length / len(words)\n",
    "            feature_list.append(unique_rate)\n",
    "            feature_list.append(average_length)\n",
    "        \n",
    "        # Time\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            month = int(re.search(r'(\\d+-\\d+-\\d+)', soup.time.text).group(1).split(\"-\")[1])\n",
    "            feature_list.append(month)\n",
    "            hour = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[0])\n",
    "            minute = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[1])\n",
    "            feature_list.append(hour * 60 + minute)\n",
    "\n",
    "        # Weekend\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            feature_list.append(1 if soup.time.get(\"datetime\")[:3] in [\"Sat\", \"Sun\"] else 0)\n",
    "\n",
    "        # Word Count\n",
    "        text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "        c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "        text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "        c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "        total = c_div + c_p\n",
    "        feature_list.append(len(list(total.elements())))\n",
    "\n",
    "        section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "\n",
    "        # Video + Image count\n",
    "        img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "        video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "        media_count = img_count + video_count\n",
    "        feature_list.append(media_count)\n",
    "\n",
    "        # Appealing count\n",
    "        link_count = len(section.find_all(\"a\"))\n",
    "        strong_count = len(section.find_all(\"strong\"))\n",
    "        appealing_count = link_count + strong_count\n",
    "        feature_list.append(appealing_count)\n",
    "\n",
    "        # POS & NEG count\n",
    "        paragraph = section.find_all(\"p\")\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        q_count = 0\n",
    "        ex_count = 0\n",
    "        for tag in paragraph:\n",
    "            pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "            neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "            if tag.text.find(\"?\") != -1:\n",
    "                q_count += tag.text.find(\"?\")\n",
    "            if tag.text.find(\"!\") != -1:\n",
    "                ex_count += tag.text.find(\"!\")\n",
    "        feature_list.append(pos_count)    \n",
    "        feature_list.append(neg_count)\n",
    "        feature_list.append(q_count)\n",
    "        feature_list.append(ex_count)\n",
    "\n",
    "        X.append(feature_list)\n",
    "    return np.array(X)\n",
    "# feature_selection_part2(X[0:10],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def feature_selection_part3(data):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "    \n",
    "    feature_str = re.sub(r'[.:\\',$()`]', '', feature_str.lower())\n",
    "    return feature_str\n",
    "\n",
    "print(feature_selection_part3(X[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random + GBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin vectorizer Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johnson\\.conda\\envs\\DL2\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=.2)\n",
    "print('Begin vectorizer Training...')\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection_part1,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "tfidf.fit(X)\n",
    "\n",
    "# tfidf_type = TfidfVectorizer(ngram_range=(1,2),\n",
    "#                         preprocessor=feature_selection_part3,\n",
    "#                         tokenizer=tokenizer_stem_nostop)\n",
    "# tfidf_type.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Begin Type Training...')\n",
    "# type_model = SGDClassifier(loss='log_loss', max_iter=200)\n",
    "# x_train_type = tfidf_type.transform(X_train)\n",
    "# y_train = LabelEncoder().fit_transform(Y_train)\n",
    "# type_model.fit(x_train_type, y_train)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "word_model = RandomForestClassifier(n_estimator=100, max_features=0.4, max_depth=5, min_samples_leaf=5)\n",
    "x_train_word = tfidf.transform(X_train)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "word_model.fit(x_train_word, y_train)\n",
    "\n",
    "print('Begin Stats Training...')\n",
    "stats_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "x_train_stats = feature_selection_part2(X_train, True)\n",
    "stats_model.fit(x_train_stats, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Begin Evaluation...')\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)\n",
    "\n",
    "# y_pred_type_train = type_model.predict_proba(x_train_type)[:,1]\n",
    "# y_pred_type_valid = type_model.predict_proba(tfidf_type.transform(X_valid))[:,1]\n",
    "\n",
    "y_pred_word_train = word_model.predict_proba(x_train_word)[:,1]\n",
    "y_pred_word_valid = word_model.predict_proba(tfidf.transform(X_valid))[:,1]\n",
    "\n",
    "y_pred_stats_train = stats_model.predict_proba(x_train_stats)[:,1]\n",
    "y_pred_stats_valid = stats_model.predict_proba(feature_selection_part2(X_valid, False))[:,1]\n",
    "\n",
    "# print(\"Type Model\")\n",
    "# print(f'Train score: {roc_auc_score(y_train, y_pred_type_train)}')\n",
    "# print(f'Valid score: {roc_auc_score(y_valid, y_pred_type_valid)}')\n",
    "\n",
    "print(\"Word Model\")\n",
    "print(f'Train score: {roc_auc_score(y_train, y_pred_word_train)}')\n",
    "print(f'Valid score: {roc_auc_score(y_valid, y_pred_word_valid)}')\n",
    "\n",
    "print(\"Stats Model\")\n",
    "print(f'Train score: {roc_auc_score(y_train, y_pred_stats_train)}')\n",
    "print(f'Valid score: {roc_auc_score(y_valid, y_pred_stats_valid)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find best para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "valid_score = []\n",
    "best_coef = []\n",
    "best_train = 0\n",
    "best_valid = 0\n",
    "coef_ = np.linspace(0,.5,21)\n",
    "print('Find best para...')\n",
    "for i in coef_:\n",
    "    # for j in coef_:\n",
    "    # train_score = roc_auc_score(y_train, y_pred_word_train*(i) + y_pred_stats_train*(1-i) + y_pred_type_train*(1-i-j))\n",
    "    # valid_score = roc_auc_score(y_valid, y_pred_word_valid*(i) + y_pred_stats_valid*(1-i) + y_pred_type_valid*(1-i-j))\n",
    "    train_score = roc_auc_score(y_train, y_pred_word_train*(i) + y_pred_stats_train*(1-i))\n",
    "    valid_score = roc_auc_score(y_valid, y_pred_word_valid*(i) + y_pred_stats_valid*(1-i))\n",
    "    if valid_score > best_valid:\n",
    "        best_valid = valid_score\n",
    "        best_train = train_score\n",
    "        best_coef = [i,1-i]\n",
    "        # best_coef = [i,j,1-i-j]\n",
    "\n",
    "print(f'Train score: {best_train}')\n",
    "print(f'Valid score: {best_valid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(word_model, open(\"output/backup_word.pickle\", \"wb\"))\n",
    "pkl.dump(stats_model, open(\"output/backup_stats.pickle\", \"wb\"))\n",
    "# pkl.dump(type_model, open('output/backup_topic.pickle', \"wb\"))\n",
    "pkl.dump(tfidf, open('output/tfidf_part1.pickle', \"wb\"))\n",
    "# pkl.dump(tfidf_type, open('output/tfidf_first_paragraph.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/test.csv')\n",
    "print(\"Content:\\n\",df.loc[0])\n",
    "\n",
    "Id = df.loc[:, 'Id'].to_numpy()\n",
    "X_test = df.loc[:, 'Page content'].to_numpy()\n",
    "print(X_test.shape)\n",
    "print(\"\\nId:\\n\", Id[0])\n",
    "print(\"Content:\\n\", X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test_type = tfidf_type.transform(X_test)\n",
    "x_test_word = tfidf.transform(X_test)\n",
    "x_test_stats = feature_selection_part2(X_test, False)\n",
    "\n",
    "# y_pred_type_test = type_model.predict_proba(x_test_type)[:,1]\n",
    "y_pred_word_test = word_model.predict_proba(x_test_word)[:,1]\n",
    "y_pred_stats_test = stats_model.predict_proba(x_test_stats)[:,1]\n",
    "y_pred = np.around(y_pred_word_test*(best_coef[0]) + y_pred_stats_test*(best_coef[1]), decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {'Id': Id, 'Popularity': y_pred}\n",
    "output_dataframe = pd.DataFrame(output_data)\n",
    "print(output_dataframe)\n",
    "\n",
    "output_dataframe.to_csv(\"./datasets/y_pred.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
