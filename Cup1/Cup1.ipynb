{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition 1. Predicting News Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "df = pd.read_csv('./datasets/train.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:\n",
      " -1\n",
      "Content:\n",
      " <html><head><div class=\"article-info\"> <span class=\"byline basic\">Clara Moskowitz</span> for <a href=\"/publishers/space-com/\">Space.com</a> <time datetime=\"Wed, 19 Jun 2013 15:04:30 +0000\">2013-06-19 15:04:30 UTC</time> </div></head><body><h1 class=\"title\">NASA's Grand Challenge: Stop Asteroids From Destroying Earth</h1><figure class=\"article-image\"><img class=\"microcontent\" data-fragment=\"lead-image\" data-image=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\" data-micro=\"1\" data-url=\"http://mashable.com/2013/06/19/nasa-grand-challenge-asteroid/\" src=\"http://i.amz.mshcdn.com/I7b9cUsPSztew7r1WT6_iBLjflo=/950x534/2013%2F06%2F19%2Ffe%2FDactyl.44419.jpg\"/></figure><article data-channel=\"world\"><section class=\"article-content\"> <p>There may be killer asteroids headed for Earth, and NASA has decided to do something about it. The space agency announced a new \"Grand Challenge\" on June 18 to find all dangerous space rocks and figure out how to stop them from destroying our planet.</p> <p>The new mission builds on projects already underway at NASA, including a plan to <a href=\"http://www.space.com/20591-nasa-asteroid-capture-mission-feasibility.html\" target=\"_blank\">capture an asteroid</a>, pull it in toward the moon and send astronauts to visit it. As part of the Grand Challenge, the agency issued a \"request for information\" today — aiming to solicit ideas from industry, academia and the public on how to improve the asteroid mission plan.</p> <p>\"We're asking for you to think about concepts and different approaches for what we've described here,\" William Gerstenmaier, NASA's associate administrator for human explorations and operations, said yesterday during a NASA event announcing the initiative. \"We want you to think about other ways of enhancing this to get the most out of it.\"</p> <p><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture</a></strong><br><br>Responses to the request for information, which also seeks ideas for detecting and mitigating asteroid threats, are due July 18.<br><br>The asteroid-retrieval mission, designed to provide the first deep-space mission for astronauts flying on NASA's Space Launch System rocket and Orion space capsule under development, has come under fire from lawmakers who would prefer that NASA return to the moon.<br><br>A <a href=\"http://www.space.com/21609-nasa-asteroid-capture-mission-congress.html\" target=\"_blank\">draft NASA authorization bill</a> from the House space subcommittee, which is currently in debate, would cancel the mission and steer the agency toward other projects. That bill will be discussed during a hearing Wednesday, June 19 at 10 a.m. EDT.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20606-nasa-asteroid-capture-mission-images.html\" target=\"_blank\">How It Works: NASA Asteroid-Capture Mission in Pictures</a></strong><br><br>But NASA officials defended the asteroid mission today and said they were confident they'd win Congress' support once they explained its benefits further.<br><br>\"I think that we really, truly are going to be able to show the value of the mission,\" NASA Associate Administrator Lori Garver said today. \"To me, this is something that what we do in this country — we debate how we spend the public's money. This is the beginning of the debate.\"<br><br>Garver also maintained that sending astronauts to an asteroid would not diminish NASA's other science and exploration goals, including another lunar landing.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20601-animation-of-proposed-asteroid-retrieval-mission-video.html\" target=\"_blank\">Animation Of Proposed Asteroid Retrieval Mission</a></strong><br><br>\"This initiative takes nothing from the other valuable work,\" she said. \"This is only a small piece of our overall strategy, but it is an integral piece. It takes nothing from the moon.\"<br><br>Part of NASA's plan to win support for the flight is to link it more closely with the larger goal of protecting Earth from asteroid threats.<br><br>If, someday, humanity discovers an asteroid headed for Earth and manages to alter its course, \"it will be one of the most important accomplishments in human history,\" said Tom Kalil, deputy director for technology and innovation at the White House Office of Science and Technology Policy.<br><br><divclass><strong>SEE ALSO: <a href=\"http://www.space.com/20006-deep-space-missions-private-companies.html\" target=\"_blank\">Wildest Private Deep-Space Mission Ideas: A Countdown</a></strong><br><br>The topic of asteroid threats is more timely than ever, after a meteor exploded over the Russian city of <a href=\"http://www.space.com/19823-russia-meteor-explosion-complete-coverage.html\" target=\"_blank\">Chelyabinsk</a> on Feb. 15 — the same day that the football field-sized <a href=\"http://www.space.com/19646-asteroid-2012-da14-earth-flyby-complete-coverage.html\" target=\"_blank\">asteroid 2012 DA14</a> passed within the moon's orbit of Earth.<br><br><em>Image courtesy of <a href=\"http://www.dvidshub.net/image/707596/ida-and-dactyl#.UcHDQvk4uSo\" target=\"_blank\">NASA</a></em></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></br></br></br></br></br></br></br></br></divclass></p> <ul> <li><a href=\"http://www.space.com/34406-spacexs-musk-says-sabotage-unlikely-cause-of-sept-1-explosion-but-still-a-worry.html\">SpaceX's Musk Says Sabotage Unlikely Cause of Sept. 1 Explosion, But Still a Worry</a></li> <li><a href=\"http://www.space.com/34405-proxima-centauri-starspots-stellar-cycle-habitable-planet-alien-life.html\">Proxima Centauri Is Like Our Sun... on Steroids</a></li> <li><a href=\"http://www.space.com/34404-china-launches-shenzhou-11-astronauts-to-space-lab.html\">China Launches Shenzhou-11 Astronauts to Tiangong-2 Space Lab</a></li> <li><a href=\"http://www.space.com/34403-space-station-mockup-in-houston-astronaut-guided-tour-video.html\">Space Station Mockup In Houston - Astronaut Guided Tour | Video</a></li> </ul> <p> This article originally published at Space.com <a href=\"http://www.space.com/21610-nasa-asteroid-threat-grand-challenge.html?\">here</a> </p> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/asteroid/\">Asteroid</a>, <a href=\"/category/asteroids/\">Asteroids</a>, <a href=\"/category/challenge/\">challenge</a>, <a href=\"/category/earth/\">Earth</a>, <a href=\"/category/space/\">Space</a>, <a href=\"/category/us/\">U.S.</a>, <a href=\"/category/world/\">World</a> </footer></body></html>\n"
     ]
    }
   ],
   "source": [
    "print(\"Label:\\n\", df.loc[0, 'Popularity'])\n",
    "print(\"Content:\\n\",df.loc[0,'Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643,) (27643,)\n"
     ]
    }
   ],
   "source": [
    "X = df.loc[:, 'Page content'].to_numpy()\n",
    "y = df.loc[:,'Popularity'].to_numpy()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Organize Positive / Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "POSITIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'positive-words.txt')\n",
    "NEGATIVE_WORDS = os.path.join(os.getcwd(), 'datasets', 'negative-words.txt')\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "\n",
    "for line in open(POSITIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    pos_words.append(word)\n",
    "\n",
    "for line in open(NEGATIVE_WORDS, 'r').readlines()[35:]:\n",
    "    word = line.rstrip()\n",
    "    neg_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "4783\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_words))\n",
    "print(len(neg_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive / Negative Word Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/winston/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_neg(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in neg_words and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def tokenizer_stem_pos(text):\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w in pos_words and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    stemmer = EnglishStemmer()\n",
    "    return [stemmer.stem(w) for w in re.split(r'\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  NASA's Grand Challenge: Stop Asteroids From Destroying Earth\n",
      "Data Channel:  world\n",
      "Author:  Clara Moskowitz\n",
      "Topics:  ['Asteroid', ' Asteroids', ' challenge', ' Earth', ' Space', ' U.S.', ' World ']\n",
      "Time:  15:04\n",
      "Weekend:  0\n",
      "Total words:  543\n",
      "Media count:  0\n",
      "Link count:  18\n",
      "Positive count:  14\n",
      "Positive words:  ['improv', 'pref', 'confid', 'win', 'support', 'benefit', 'valu', 'integr', 'win', 'support', 'import', 'accompl', 'innov', 'tim']\n",
      "Negative count:  3\n",
      "Negative words:  ['kil', 'dang', 'threats']\n",
      "Question count:  0\n",
      "Exclamation count:  0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "def feature_selection(data):\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    print(\"Title: \", soup.find(\"h1\", {\"class\": \"title\"}).text)\n",
    "\n",
    "    # Channel\n",
    "    print(\"Data Channel: \", soup.find(\"article\").get(\"data-channel\"))\n",
    "\n",
    "    # Author\n",
    "    author_re = r'(?:By\\s)?([a-zA-Z]+(\\s[a-zA-Z]+)*)'\n",
    "    if soup.head.find(\"span\") == None:\n",
    "        print(\"Author: None\")\n",
    "    else:\n",
    "        print(\"Author: \", re.search(author_re, soup.head.find(\"span\").text).group(1))\n",
    "\n",
    "    # Related Topics\n",
    "    print(\"Topics: \", soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\"))\n",
    "\n",
    "    # Time\n",
    "    print(\"Time: \", re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5])\n",
    "\n",
    "    # Weekend\n",
    "    print(\"Weekend: \", int(soup.time.get(\"datetime\")[:3] in [\"Sat\", \"Sun\"]))\n",
    "\n",
    "    # Word Count\n",
    "    text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "    c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "    text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "    c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "    total = c_div + c_p\n",
    "    print(\"Total words: \", len(list(total.elements())))\n",
    "\n",
    "\n",
    "    section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "\n",
    "    # Video + Image count\n",
    "    img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "    video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "    media_count = img_count + video_count\n",
    "    print(\"Media count: \", media_count)\n",
    "\n",
    "    # Appealing count\n",
    "    link_count = len(section.find_all(\"a\"))\n",
    "    strong_count = len(section.find_all(\"strong\"))\n",
    "    appealing_count = link_count + strong_count\n",
    "    print(\"Link count: \", appealing_count)\n",
    "\n",
    "    # POS & NEG count\n",
    "    paragraph = section.find_all(\"p\")\n",
    "    pos_count = 0\n",
    "    pos_words = []\n",
    "    for tag in paragraph:\n",
    "        pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "        for word in tokenizer_stem_pos(tag.text):\n",
    "            pos_words.append(word)\n",
    "    print(\"Positive count: \", pos_count)\n",
    "    print(\"Positive words: \", pos_words)\n",
    "\n",
    "    neg_count = 0\n",
    "    neg_words = []\n",
    "    for tag in paragraph:\n",
    "        neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "        for word in tokenizer_stem_neg(tag.text):\n",
    "            neg_words.append(word)\n",
    "    print(\"Negative count: \", neg_count)\n",
    "    print(\"Negative words: \", neg_words)\n",
    "\n",
    "    # Question & Exclamation count\n",
    "    q_count = 0\n",
    "    ex_count = 0\n",
    "    for tag in paragraph:\n",
    "        if tag.text.find(\"?\") != -1:\n",
    "            q_count += tag.text.find(\"?\")\n",
    "        if tag.text.find(\"!\") != -1:\n",
    "            ex_count += tag.text.find(\"!\")\n",
    "    \n",
    "    print(\"Question count: \", q_count)\n",
    "    print(\"Exclamation count: \", ex_count)\n",
    "\n",
    "\n",
    "feature_selection(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Vector Features (Word + Stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasas grand challenge stop asteroids from destroying earth world moskowitz asteroid asteroids challenge earth space us world afternoon weekday wordmedium nomedia manyappeal somestats improv pref confid win support benefit valu integr win support import accompl innov tim kil dang threats \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def feature_selection_all(data):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    feature_str += soup.find(\"h1\", {\"class\": \"title\"}).text + \" \"\n",
    "\n",
    "    # Channel\n",
    "    feature_str += soup.find(\"article\").get(\"data-channel\") + \" \"\n",
    "\n",
    "    # Author\n",
    "    author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "    if soup.head.find(\"span\") == None:\n",
    "        feature_str += \" \"\n",
    "    else:\n",
    "        author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "        feature_str += author.split(\" \")[-1] + \" \"\n",
    "\n",
    "    # Related Topics\n",
    "    feature_str += soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").replace(\",\", \"\")\n",
    "\n",
    "    # Time\n",
    "    if soup.time.text == None or soup.time.text == \"\":\n",
    "        feature_str += \"0\"\n",
    "    else:\n",
    "        hour = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[0])\n",
    "        minute = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[1])\n",
    "        if hour * 60 + minute < 540:\n",
    "            feature_str += \"morning \"\n",
    "        elif hour * 60 + minute < 720:\n",
    "            feature_str += \"noon \"\n",
    "        elif hour * 60 + minute < 1020:\n",
    "            feature_str += \"afternoon \"\n",
    "        else:\n",
    "            feature_str += \"night \"\n",
    "\n",
    "    # Weekend\n",
    "    if soup.time.text == None or soup.time.text == \"\":\n",
    "        feature_str += \"weekday \"\n",
    "    else:\n",
    "        feature_str += \"weekend \" if soup.time.get(\"datetime\")[:3] in [\"Sat\", \"Sun\"] else \"weekday \"\n",
    "\n",
    "    # Word Count\n",
    "    text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "    c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "    text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "    c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "    total = c_div + c_p\n",
    "    if len(list(total.elements())) < 300:\n",
    "        feature_str += 'wordshort '\n",
    "    elif len(list(total.elements())) < 600:\n",
    "        feature_str += 'wordmedium '\n",
    "    else:\n",
    "        feature_str += 'wordlong '\n",
    "\n",
    "\n",
    "    section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "\n",
    "    # Video + Image count\n",
    "    img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "    video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "    media_count = img_count + video_count\n",
    "    if media_count < 3:\n",
    "        feature_str += \"nomedia \"\n",
    "    elif media_count < 7:\n",
    "        feature_str += \"fewmedia \"\n",
    "    else:\n",
    "        feature_str += \"manymedia \"\n",
    "\n",
    "    # Appealing count\n",
    "    link_count = len(section.find_all(\"a\"))\n",
    "    strong_count = len(section.find_all(\"strong\"))\n",
    "    appealing_count = link_count + strong_count\n",
    "    if appealing_count < 3:\n",
    "        feature_str += \"noappeal \"\n",
    "    elif appealing_count < 10:\n",
    "        feature_str += \"fewappeal \"\n",
    "    else:\n",
    "        feature_str += \"manyappeal \"\n",
    "\n",
    "    paragraph = section.find_all(\"p\")\n",
    "    # Statistics count\n",
    "    stat_count = 0\n",
    "    for tag in paragraph:\n",
    "        number = r'\\d+'\n",
    "        matches = re.findall(number, tag.text)\n",
    "        stat_count += len(matches)\n",
    "    if stat_count < 5:\n",
    "        feature_str += \"fewstats \"\n",
    "    elif stat_count < 10:\n",
    "        feature_str += \"somestats \"\n",
    "    elif stat_count < 20:\n",
    "        feature_str += \"manystats \"\n",
    "    else:\n",
    "        feature_str += \"lotstats \"\n",
    "\n",
    "    # POS & NEG words\n",
    "    pos_count = 0\n",
    "    for tag in paragraph:\n",
    "        pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "        for word in tokenizer_stem_pos(tag.text):\n",
    "            feature_str += word + \" \"\n",
    "\n",
    "\n",
    "    neg_count = 0\n",
    "    for tag in paragraph:\n",
    "        neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "        for word in tokenizer_stem_neg(tag.text):\n",
    "            feature_str += word + \" \"\n",
    "    \n",
    "\n",
    "    feature_str = re.sub(r'[,-.:\\']+', '', feature_str.lower())\n",
    "    return feature_str\n",
    "\n",
    "print(feature_selection_all(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Part 1 (Only Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasas grand challenge stop asteroids from destroying earth world moskowitz improv pref confid win support benefit valu integr win support import accompl innov tim kil dang threats asteroid asteroids challenge earth space us world \n"
     ]
    }
   ],
   "source": [
    "def feature_selection_part1(data):\n",
    "    feature_str = \"\"\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    feature_str += soup.find(\"h1\", {\"class\": \"title\"}).text + \" \"\n",
    "\n",
    "    # Channel\n",
    "    feature_str += soup.find(\"article\").get(\"data-channel\") + \" \"\n",
    "\n",
    "    # Author\n",
    "    author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "    if soup.head.find(\"span\") == None:\n",
    "        feature_str += \" \"\n",
    "    else:\n",
    "        author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "        feature_str += author.split(\" \")[-1] + \" \"\n",
    "\n",
    "    section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "    paragraph = section.find_all(\"p\")\n",
    "    pos_count = 0\n",
    "    for tag in paragraph:\n",
    "        pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "        for word in tokenizer_stem_pos(tag.text):\n",
    "            feature_str += word + \" \"\n",
    "\n",
    "    neg_count = 0\n",
    "    for tag in paragraph:\n",
    "        neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "        for word in tokenizer_stem_neg(tag.text):\n",
    "            feature_str += word + \" \"\n",
    "\n",
    "    # Related Topics\n",
    "    feature_str += soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").replace(\",\", \"\")\n",
    "\n",
    "    feature_str = re.sub(r'[.:\\']', '', feature_str.lower())\n",
    "    return feature_str\n",
    "\n",
    "print(feature_selection_part1(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Part 2 (Basic stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.00000000e+00 9.04000000e+02 0.00000000e+00 5.43000000e+02\n",
      "  0.00000000e+00 0.00000000e+00 1.80000000e+01 7.00000000e+00\n",
      "  1.40000000e+01 3.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  6.00000000e+01 4.89731438e-01 4.81042654e+00]\n",
      " [3.00000000e+00 1.06000000e+03 0.00000000e+00 3.21000000e+02\n",
      "  0.00000000e+00 0.00000000e+00 9.00000000e+00 2.00000000e+00\n",
      "  5.00000000e+00 3.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  7.40000000e+01 4.74719101e-01 4.65449438e+00]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def feature_selection_part2(data):\n",
    "    X = []\n",
    "    for html in data:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        feature_list = []\n",
    "        # Time\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            month = int(re.search(r'(\\d+-\\d+-\\d+)', soup.time.text).group(1).split(\"-\")[1])\n",
    "            feature_list.append(month)\n",
    "            hour = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[0])\n",
    "            minute = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[1])\n",
    "            feature_list.append(hour * 60 + minute)\n",
    "\n",
    "        # Weekend\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            feature_list.append(1 if soup.time.get(\"datetime\")[:3] in [\"Sat\", \"Sun\"] else 0)\n",
    "\n",
    "        # Word Count\n",
    "        text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "        c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "        text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "        c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "        total = c_div + c_p\n",
    "        feature_list.append(len(list(total.elements())))\n",
    "\n",
    "        # Script count\n",
    "        script = soup.find_all(\"script\")\n",
    "        feature_list.append(len(script))\n",
    "\n",
    "        section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "\n",
    "        # Video + Image count\n",
    "        img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "        video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "        media_count = img_count + video_count\n",
    "        feature_list.append(media_count)\n",
    "\n",
    "        # Appealing count\n",
    "        link_count = len(section.find_all(\"a\"))\n",
    "        strong_count = len(section.find_all(\"strong\"))\n",
    "        h1_h2_count = len(section.find_all(\"h1\")) + len(section.find_all(\"h2\"))\n",
    "        table_count = len(section.find_all(\"table\")) + len(section.find_all(\"tr\")) + len(section.find_all(\"td\"))\n",
    "        appealing_count = link_count + strong_count + h1_h2_count + table_count\n",
    "        feature_list.append(appealing_count)\n",
    "\n",
    "        paragraph = section.find_all(\"p\")\n",
    "        # Statistics count\n",
    "        stat_count = 0\n",
    "        for tag in paragraph:\n",
    "            number = r'\\d+'\n",
    "            matches = re.findall(number, tag.text)\n",
    "            stat_count += len(matches)\n",
    "        feature_list.append(stat_count)\n",
    "\n",
    "        # POS & NEG count\n",
    "        pos_count = 0\n",
    "        for tag in paragraph:\n",
    "            pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "        feature_list.append(pos_count)\n",
    "\n",
    "        neg_count = 0\n",
    "        for tag in paragraph:\n",
    "            neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "        feature_list.append(neg_count)\n",
    "\n",
    "        # Question & Exclamation count\n",
    "        q_count = 0\n",
    "        for tag in paragraph:\n",
    "            if tag.text.find(\"?\") != -1:\n",
    "                q_count += tag.text.find(\"?\")\n",
    "        feature_list.append(q_count)\n",
    "\n",
    "        ex_count = 0\n",
    "        for tag in paragraph:\n",
    "            if tag.text.find(\"!\") != -1:\n",
    "                ex_count += tag.text.find(\"!\")\n",
    "        feature_list.append(ex_count)\n",
    "\n",
    "        # word count of title\n",
    "        feature_list.append(len(soup.find(\"h1\", {\"class\": \"title\"}).text))\n",
    "        \n",
    "        # average word length and unique word rate\n",
    "        words = re.findall(r'\\w+', soup.get_text().lower())\n",
    "        if words:\n",
    "            total_words = len(words)\n",
    "            unique_words = set(words)\n",
    "            unique_word_count = len(unique_words)\n",
    "            total_length = sum(len(word) for word in words)\n",
    "            unique_rate = unique_word_count / total_words\n",
    "            average_length = total_length / len(words)\n",
    "            feature_list.append(unique_rate)\n",
    "            feature_list.append(average_length)\n",
    "            \n",
    "\n",
    "        X.append(feature_list)\n",
    "    \n",
    "    return np.array(X)\n",
    "\n",
    "print(feature_selection_part2(X[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection Part 3 (Johnson's Version => Advance Stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  6.00000000e+01\n",
      "   4.89731438e-01  4.81042654e+00  6.00000000e+00  1.00000000e+00\n",
      "   3.00000000e+00  0.00000000e+00  3.31491713e-02  2.57826888e-02\n",
      "   5.52486188e-03  0.00000000e+00  0.00000000e+00  1.08600000e+02\n",
      "  -3.61200000e-01  9.66700000e-01]\n",
      " [ 1.00000000e+00  1.00000000e+00  1.00000000e+00  7.40000000e+01\n",
      "   4.74719101e-01  4.65449438e+00  3.00000000e+00  2.00000000e+00\n",
      "   4.00000000e+00  0.00000000e+00  2.80373832e-02  1.55763240e-02\n",
      "   9.34579439e-03  0.00000000e+00  0.00000000e+00  4.01250000e+01\n",
      "   3.57000000e-01  9.89300000e-01]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "author_score =dict()\n",
    "author_num = dict()\n",
    "channel_score = dict()\n",
    "channel_num = dict()\n",
    "topic_score = dict()\n",
    "topic_num = dict()\n",
    "avg_author = 0\n",
    "avg_channel = 0\n",
    "avg_topic = 0\n",
    "\n",
    "def feature_selection_part3(data, istraining):\n",
    "    X = []\n",
    "    idx=0\n",
    "    global author_score\n",
    "    global author_num \n",
    "    global channel_score \n",
    "    global channel_num \n",
    "    global topic_score \n",
    "    global topic_num \n",
    "    global avg_author \n",
    "    global avg_channel \n",
    "    global avg_topic \n",
    "    \n",
    "    if(istraining):\n",
    "        for html in data:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            channel = soup.find(\"article\").get(\"data-channel\")\n",
    "            if channel in channel_score:\n",
    "                channel_score[channel] += 1 if int(y[idx])==1 else  0\n",
    "                channel_num[channel] += 1\n",
    "            else : \n",
    "                channel_score[channel] = 0\n",
    "                channel_score[channel] += 1 if int(y[idx])==1 else  0\n",
    "                channel_num[channel] = 1\n",
    "            \n",
    "            topics = soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\")\n",
    "            for topic in topics:\n",
    "                if topic in topic_score:\n",
    "                    topic_score[topic] += 1 if int(y[idx])==1 else  0\n",
    "                    topic_num[topic] += 1\n",
    "                else : \n",
    "                    topic_score[topic] = 0\n",
    "                    topic_score[topic] += 1 if int(y[idx])==1 else  0\n",
    "                    topic_num[topic] = 1\n",
    "\n",
    "\n",
    "            # Author\n",
    "            author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "            if soup.head.find(\"span\") == None:\n",
    "                continue\n",
    "            else:\n",
    "                author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "                if author in author_score:\n",
    "                    author_score[author] += 1 if int(y[idx])==1 else  0\n",
    "                    author_num[author] += 1\n",
    "                else : \n",
    "                    author_score[author] = 0\n",
    "                    author_num[author] = 1\n",
    "                    author_score[author] += 1 if int(y[idx])==1 else  0\n",
    "            idx+=1\n",
    "        total_channel = 0\n",
    "        total_topic = 0\n",
    "        total_author = 0\n",
    "        for c,s in channel_score.items():\n",
    "            avg_channel+=s\n",
    "            total_channel+=channel_num[c]\n",
    "        avg_channel = avg_channel/total_channel\n",
    "\n",
    "        for c,s in topic_score.items():\n",
    "            avg_topic+=s\n",
    "            total_topic+=topic_num[c]\n",
    "        avg_topic = avg_topic/total_topic\n",
    "\n",
    "        for c,s in author_score.items():\n",
    "            avg_author+=s\n",
    "            total_author+=author_num[c]\n",
    "        avg_author = avg_author/total_author\n",
    "\n",
    "    for html in data:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        feature_list = []\n",
    "        # Author score\n",
    "        author_re = r'(?:By\\s|by\\s)?([a-zA-Z]+(\\s[A-Z][a-z]+)*)'\n",
    "        if soup.head.find(\"span\") == None:\n",
    "            feature_list.append(0.5)\n",
    "        else:\n",
    "            author = re.search(author_re, soup.head.find(\"span\").text).group(1)\n",
    "            if author in author_score:\n",
    "                feature_list.append(author_score[author]/author_num[author])\n",
    "            else:\n",
    "                feature_list.append(0.5)\n",
    "        # Channel score\n",
    "        channel = soup.find(\"article\").get(\"data-channel\")\n",
    "        if channel in channel_score:\n",
    "            feature_list.append(channel_score[channel]/channel_num[channel])\n",
    "        else:\n",
    "            feature_list.append(0.5)\n",
    "            \n",
    "        # Topic score\n",
    "        topics = soup.find(\"footer\", {\"class\": 'article-topics'}).text.replace(\" Topics: \", \"\").split(\",\")\n",
    "        total_score = 0\n",
    "        order = 0 \n",
    "        order_denominator = 0\n",
    "        for i in range(len(topics)):\n",
    "            order_denominator += math.exp(-0.5*i)\n",
    "\n",
    "        for topic in topics:\n",
    "            order_coef = math.exp(-0.5*order)/order_denominator\n",
    "            if topic in topic_score:\n",
    "                total_score+=(topic_score[topic]/topic_num[topic])*order_coef\n",
    "            else:\n",
    "                total_score+=0.5*order_coef\n",
    "            order+=1\n",
    "        \n",
    "        feature_list.append(total_score) \n",
    "\n",
    "        # word count of title\n",
    "        title = soup.find(\"h1\", {\"class\": \"title\"}).text\n",
    "        feature_list.append(len(title))\n",
    "        # average word length and unique word rate\n",
    "        words = re.findall(r'\\w+', soup.get_text().lower())\n",
    "        if words:\n",
    "            total_words = len(words)\n",
    "            unique_words = set(words)\n",
    "            unique_word_count = len(unique_words)\n",
    "            total_length = sum(len(word) for word in words)\n",
    "            unique_rate = unique_word_count / total_words\n",
    "            average_length = total_length / len(words)\n",
    "            feature_list.append(unique_rate)\n",
    "            feature_list.append(average_length)\n",
    "        \n",
    "        # Time\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            month = int(re.search(r'(\\d+-\\d+-\\d+)', soup.time.text).group(1).split(\"-\")[1])\n",
    "            feature_list.append(month)\n",
    "            hour = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[0])\n",
    "            minute = int(re.search(r'(\\d+:\\d+:\\d+)', soup.time.text).group(1)[:5].split(\":\")[1])\n",
    "            time = (hour * 60 + minute-300)%1440\n",
    "            time_demarcation = 0\n",
    "            if(300<=time and time<=720):\n",
    "                time_demarcation = 1\n",
    "            elif(720<time and time<=960):\n",
    "                time_demarcation = 2\n",
    "            elif(960<time and time <=1440):\n",
    "                time_demarcation = 3\n",
    "            else:\n",
    "                time_demarcation = 4\n",
    "            feature_list.append(time_demarcation)\n",
    "\n",
    "        # Weekday\n",
    "        if soup.time.text == None or soup.time.text == \"\":\n",
    "            feature_list.append(0)\n",
    "        else:\n",
    "            weekday = soup.time.get(\"datetime\")[:3]\n",
    "            if(weekday == \"Mon\"):\n",
    "                feature_list.append(1)\n",
    "            elif(weekday == \"Tue\"):\n",
    "                feature_list.append(2)\n",
    "            elif(weekday == \"Wed\"):\n",
    "                feature_list.append(3)\n",
    "            elif(weekday == \"Thu\"):\n",
    "                feature_list.append(4)\n",
    "            elif(weekday == \"Fri\"):\n",
    "                feature_list.append(5)\n",
    "            elif(weekday==\"Sat\"):\n",
    "                feature_list.append(6)\n",
    "            elif(weekday==\"Sun\"):\n",
    "                feature_list.append(7)\n",
    "            else: feature_list.append(0)\n",
    "\n",
    "        # Word Count\n",
    "        text_p = (''.join(s.findAll(string=True))for s in soup.findAll('p'))\n",
    "        c_p = Counter((x.rstrip(punctuation).lower() for y in text_p for x in y.split()))\n",
    "        text_div = (''.join(s.findAll(string=True))for s in soup.findAll('div'))\n",
    "        c_div = Counter((x.rstrip(punctuation).lower() for y in text_div for x in y.split()))\n",
    "        total = c_div + c_p\n",
    "        word_count = len(list(total.elements()))\n",
    "\n",
    "        section = soup.find(\"section\", {\"class\": \"article-content\"})\n",
    "\n",
    "        # Video + Image count\n",
    "        img_count = len(section.find_all(\"img\")) + len(section.find_all(\"picture\")) + len(section.find_all(\"figure\"))\n",
    "        video_count = len(section.find_all(\"video\")) + len(section.find_all(\"iframe\"))\n",
    "        media_count = img_count + video_count\n",
    "        feature_list.append(media_count / word_count)\n",
    "\n",
    "        # Appealing count\n",
    "        link_count = len(section.find_all(\"a\"))\n",
    "        strong_count = len(section.find_all(\"strong\"))\n",
    "        appealing_count = link_count + strong_count\n",
    "        feature_list.append(appealing_count / word_count)\n",
    "\n",
    "        # POS & NEG count\n",
    "        paragraph = section.find_all(\"p\")\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        q_count = 0\n",
    "        ex_count = 0\n",
    "        para_count = 0\n",
    "        content=\"\"\n",
    "        for tag in paragraph:\n",
    "            content+=tag.text\n",
    "            para_count += 1\n",
    "            pos_count += len(tokenizer_stem_pos(tag.text))\n",
    "            neg_count += len(tokenizer_stem_neg(tag.text))\n",
    "            if tag.text.find(\"?\") != -1:\n",
    "                q_count += tag.text.find(\"?\")\n",
    "            if tag.text.find(\"!\") != -1:\n",
    "                ex_count += tag.text.find(\"!\")\n",
    "        word_cnt = len(list(total.elements()))\n",
    "        feature_list.append(pos_count/word_cnt)    \n",
    "        feature_list.append(neg_count/word_cnt)\n",
    "        feature_list.append(q_count/word_cnt)\n",
    "        feature_list.append(ex_count/word_cnt)\n",
    "        feature_list.append(word_cnt/para_count)\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        # Title sentiment analysis\n",
    "        sentiment_title = analyzer.polarity_scores(title)\n",
    "        feature_list.append(sentiment_title['compound'])\n",
    "        # Content sentiment analysis\n",
    "        sentiment_content = analyzer.polarity_scores(content)\n",
    "        feature_list.append(sentiment_content['compound'])\n",
    "        X.append(feature_list)\n",
    "    return np.array(X)\n",
    "\n",
    "print(feature_selection_part3(X[:2], True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EmilyXia\\anaconda3\\envs\\dl\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "nomed: 1.58\n",
      "weekday: 1.69\n",
      "appeallong: 1.83\n",
      "nightweekday: 2.00\n",
      "wordlong: 2.00\n",
      "appealmed: 2.04\n",
      "wordshort: 2.11\n",
      "manymed: 2.14\n",
      "wordmed: 2.19\n",
      "afternoon: 2.23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=feature_selection_all,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "tfidf.fit(X)\n",
    "\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    # When sklearn version <= 0.24.x, should use get_feature_names()\n",
    "    # When sklearn version >= 1.0.x, should use get_feature_names_out()\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names_out()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "# doc_tfidf = tfidf.transform(X).toarray()\n",
    "# tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "# print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "# for tok, v in zip(tfidf.inverse_transform(np.ones((1, tfidf_sum.shape[0])))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "#                         np.sort(tfidf_sum)[::-1][:top]):\n",
    "#     print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# hash words to 1024 buckets\n",
    "hashvec = HashingVectorizer(n_features=2**20,\n",
    "                            preprocessor=feature_selection,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "# no .fit needed for HashingVectorizer, since it's defined by the hash function\n",
    "\n",
    "# transform sentences to vectors of dimension 1024\n",
    "# X_hash = hashvec.transform(X)\n",
    "# print(X_hash.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Popularity                                       Page content\n",
      "0   0          -1  <html><head><div class=\"article-info\"> <span c...\n",
      "1   1           1  <html><head><div class=\"article-info\"><span cl...\n",
      "2   2           1  <html><head><div class=\"article-info\"><span cl...\n",
      "3   3          -1  <html><head><div class=\"article-info\"><span cl...\n",
      "4   4          -1  <html><head><div class=\"article-info\"><span cl...\n"
     ]
    }
   ],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "print(next(get_stream(path='./datasets/train.csv', size=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 1. XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "batch_size = 512\n",
    "stream = get_stream(path='./datasets/train.csv', size=batch_size)\n",
    "train_auc, val_auc = [], []\n",
    "iters = int((27643+batch_size-1)/(batch_size*2))\n",
    "for i in range(iters):\n",
    "    batch = next(stream)\n",
    "    X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "    if X_train is None:\n",
    "        break\n",
    "    X_train = tfidf.transform(X_train)\n",
    "    y_train = LabelEncoder().fit_transform(y_train)\n",
    "    if i == 0:\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train, xgb_model='tmp.json')\n",
    "    train_score = roc_auc_score(y_train, model.predict_proba(X_train)[:,1])\n",
    "    train_auc.append(train_score)\n",
    "    model.save_model('tmp.json')\n",
    "\n",
    "    batch = next(stream)\n",
    "    X_valid, y_valid = batch['Page content'], batch['Popularity']\n",
    "    y_valid = LabelEncoder().fit_transform(y_valid)\n",
    "    valid_score = roc_auc_score(y_valid, model.predict_proba(tfidf.transform(X_valid))[:,1])\n",
    "    val_auc.append(valid_score)\n",
    "    print(f'[{(i+1)*batch_size}/{X_train.shape[0]}]')\n",
    "    print(f'Train score: {train_score}')\n",
    "    print(f'Valid score: {valid_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 1. XGBClassifier (Part 1) + XGBClassifier (Part 2) + Propotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Word Training...\n",
      "Begin Stats Training...\n",
      "Begin Evaluation...\n",
      "Propotion = 0.1\n",
      "Stats train score: 0.8337489746948469\n",
      "Stats valid score: 0.5381647821363221\n",
      "Propotion = 0.2\n",
      "Stats train score: 0.8538771468400634\n",
      "Stats valid score: 0.5424207121841751\n",
      "Propotion = 0.3\n",
      "Stats train score: 0.8716052293672272\n",
      "Stats valid score: 0.546162726998858\n",
      "Propotion = 0.4\n",
      "Stats train score: 0.8852657226003768\n",
      "Stats valid score: 0.5490572462571346\n",
      "Propotion = 0.5\n",
      "Stats train score: 0.8934468680814405\n",
      "Stats valid score: 0.5502538298920947\n",
      "Propotion = 0.6\n",
      "Stats train score: 0.895002950718581\n",
      "Stats valid score: 0.5494158876364323\n",
      "Propotion = 0.7\n",
      "Stats train score: 0.8887981647937868\n",
      "Stats valid score: 0.5471412859894432\n",
      "Propotion = 0.8\n",
      "Stats train score: 0.873531289236538\n",
      "Stats valid score: 0.5440122528279733\n",
      "Propotion = 0.9\n",
      "Stats train score: 0.8483147518254196\n",
      "Stats valid score: 0.5401605713554354\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "word_model = xgb.XGBClassifier(max_depth=10)\n",
    "x_train_word = tfidf.transform(X_train)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "word_model.fit(x_train_word, y_train)\n",
    "\n",
    "\n",
    "print('Begin Stats Training...')\n",
    "stats_model = xgb.XGBClassifier(max_depth=10)\n",
    "x_train_stats = feature_selection_part2(X_train)\n",
    "stats_model.fit(x_train_stats, y_train)\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)\n",
    "y_pred_word_train = word_model.predict_proba(x_train_word)[:,1]\n",
    "y_pred_stats_train = stats_model.predict_proba(x_train_stats)[:,1]\n",
    "y_pred_word_valid = word_model.predict_proba(tfidf.transform(X_valid))[:,1]\n",
    "y_pred_stats_valid = stats_model.predict_proba(feature_selection_part2(X_valid))[:,1]\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    print(f'Propotion = {i}')\n",
    "    train_score = roc_auc_score(y_train, y_pred_word_train*(i) + y_pred_stats_train*(1-i))\n",
    "    valid_score = roc_auc_score(y_valid, y_pred_word_valid*(i) + y_pred_stats_valid*(1-i))\n",
    "    print(f'Stats train score: {train_score}')\n",
    "    print(f'Stats valid score: {valid_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Appoach 2. XGBClassifier (Part 1) + GradientBoostingClassifier (Part 2) + Propotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Word Training...\n",
      "Begin Stats Training...\n",
      "Begin Evaluation...\n",
      "Best coef: 0.295\n",
      "Train score: 0.8103657120014685\n",
      "Valide score: 0.5659891770926032\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "word_model = xgb.XGBClassifier()\n",
    "x_train_word = tfidf.transform(X_train)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "word_model.fit(x_train_word, y_train)\n",
    "\n",
    "\n",
    "print('Begin Stats Training...')\n",
    "stats_model = HistGradientBoostingClassifier()\n",
    "x_train_stats = feature_selection_part2(X_train)\n",
    "stats_model.fit(x_train_stats, y_train)\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)\n",
    "y_pred_word_train = word_model.predict_proba(x_train_word)[:,1]\n",
    "y_pred_stats_train = stats_model.predict_proba(x_train_stats)[:,1]\n",
    "y_pred_word_valid = word_model.predict_proba(tfidf.transform(X_valid))[:,1]\n",
    "y_pred_stats_valid = stats_model.predict_proba(feature_selection_part2(X_valid))[:,1]\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "best_coef = 0\n",
    "best_train = 0\n",
    "best_valid = 0\n",
    "coef_ = np.linspace(0,.5,101)\n",
    "for i in coef_:\n",
    "    train_score = roc_auc_score(y_train, y_pred_word_train*(i) + y_pred_stats_train*(1-i))\n",
    "    valid_score = roc_auc_score(y_valid, y_pred_word_valid*(i) + y_pred_stats_valid*(1-i))\n",
    "    if valid_score > best_valid:\n",
    "        best_valid = valid_score\n",
    "        best_train = train_score\n",
    "        best_coef = i\n",
    "\n",
    "print(f'Best coef: {best_coef}')\n",
    "print(f'Train score: {best_train}')\n",
    "print(f'Valide score: {best_valid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 3. RandomForestClassifier (Part 1) + GradientBoostingClassifier (Part 2) + Propotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Word Training...\n",
      "Begin Saving...\n",
      "Begin Stats Training...\n",
      "Begin Saving...\n",
      "Begin Evaluation...\n",
      "Propotion = 0.0\n",
      "Propotion = 0.025\n",
      "Propotion = 0.05\n",
      "Propotion = 0.07500000000000001\n",
      "Propotion = 0.1\n",
      "Propotion = 0.125\n",
      "Propotion = 0.15000000000000002\n",
      "Propotion = 0.17500000000000002\n",
      "Propotion = 0.2\n",
      "Propotion = 0.225\n",
      "Propotion = 0.25\n",
      "Propotion = 0.275\n",
      "Propotion = 0.30000000000000004\n",
      "Propotion = 0.325\n",
      "Propotion = 0.35000000000000003\n",
      "Propotion = 0.375\n",
      "Propotion = 0.4\n",
      "Propotion = 0.42500000000000004\n",
      "Propotion = 0.45\n",
      "Propotion = 0.47500000000000003\n",
      "Propotion = 0.5\n",
      "Train score: 0.9980189735758218\n",
      "Valide score: 0.5762409434051226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "word_model = RandomForestClassifier()\n",
    "x_train_word = tfidf.transform(X_train)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "word_model.fit(x_train_word, y_train)\n",
    "\n",
    "print('Begin Saving...')\n",
    "import pickle\n",
    "filename = \"backup_word.pickle\"\n",
    "# save model\n",
    "pickle.dump(word_model, open(filename, \"wb\"))\n",
    "\n",
    "print('Begin Stats Training...')\n",
    "stats_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "x_train_stats = feature_selection_part2(X_train)\n",
    "stats_model.fit(x_train_stats, y_train)\n",
    "\n",
    "print('Begin Saving...')\n",
    "import pickle\n",
    "filename = \"backup_stats.pickle\"\n",
    "# save model\n",
    "pickle.dump(stats_model, open(filename, \"wb\"))\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)\n",
    "y_pred_word_train = word_model.predict_proba(x_train_word)[:,1]\n",
    "y_pred_stats_train = stats_model.predict_proba(x_train_stats)[:,1]\n",
    "y_pred_word_valid = word_model.predict_proba(tfidf.transform(X_valid))[:,1]\n",
    "y_pred_stats_valid = stats_model.predict_proba(feature_selection_part2(X_valid))[:,1]\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "best_coef = 0\n",
    "best_train = 0\n",
    "best_valid = 0\n",
    "coef_ = np.linspace(0,.5,21)\n",
    "for i in coef_:\n",
    "    train_score = roc_auc_score(y_train, y_pred_word_train*(i) + y_pred_stats_train*(1-i))\n",
    "    valid_score = roc_auc_score(y_valid, y_pred_word_valid*(i) + y_pred_stats_valid*(1-i))\n",
    "    if valid_score > best_valid:\n",
    "        best_valid = valid_score\n",
    "        best_train = train_score\n",
    "        best_coef = i\n",
    "\n",
    "print(f'Best coef: {best_coef}') # 0.35\n",
    "print(f'Train score: {best_train}')\n",
    "print(f'Valide score: {best_valid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 4. GradientBoostingClassifier (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Word Training...\n",
      "Begin Saving...\n",
      "Begin Stats Training...\n",
      "Begin Saving...\n",
      "Begin Evaluation...\n",
      "Best coef: 0.325\n",
      "Train score: 0.9942460362600725\n",
      "Valide score: 0.5714620792381162\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Begin Stats Training...')\n",
    "stats_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "x_train_stats = feature_selection_part2(X_train)\n",
    "stats_model.fit(x_train_stats, y_train)\n",
    "\n",
    "print('Begin Saving...')\n",
    "import pickle\n",
    "filename = \"backup_stats.pickle\"\n",
    "# save model\n",
    "pickle.dump(stats_model, open(filename, \"wb\"))\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)\n",
    "y_pred_word_train = word_model.predict_proba(x_train_word)[:,1]\n",
    "y_pred_stats_train = stats_model.predict_proba(x_train_stats)[:,1]\n",
    "y_pred_word_valid = word_model.predict_proba(tfidf.transform(X_valid))[:,1]\n",
    "y_pred_stats_valid = stats_model.predict_proba(feature_selection_part2(X_valid))[:,1]\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "best_coef = 0\n",
    "best_train = 0\n",
    "best_valid = 0\n",
    "coef_ = np.linspace(0,.5,21)\n",
    "for i in coef_:\n",
    "    train_score = roc_auc_score(y_train, y_pred_word_train*(i) + y_pred_stats_train*(1-i))\n",
    "    valid_score = roc_auc_score(y_valid, y_pred_word_valid*(i) + y_pred_stats_valid*(1-i))\n",
    "    if valid_score > best_valid:\n",
    "        best_valid = valid_score\n",
    "        best_train = train_score\n",
    "        best_coef = i\n",
    "\n",
    "print(f'Best coef: {best_coef}')\n",
    "print(f'Train score: {best_train}')\n",
    "print(f'Valide score: {best_valid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 5. Voting Classifier (All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Word Training...\n",
      "Begin Saving...\n",
      "Begin Evaluation...\n",
      "Stats train score: 0.9996908653201391\n",
      "Stats valid score: 0.5492559122616445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=.2)\n",
    "word_model = RandomForestClassifier()\n",
    "stats_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "voting = VotingClassifier(estimators=[('rf', word_model), ('gbc', stats_model)], voting='soft')\n",
    "\n",
    "print('Begin Word Training...')\n",
    "x_train = tfidf.transform(X_train)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)\n",
    "voting = voting.fit(x_train, y_train)\n",
    "\n",
    "print('Begin Saving...')\n",
    "import pickle\n",
    "filename = \"backup_voting.pickle\"\n",
    "# save model\n",
    "pickle.dump(voting, open(filename, \"wb\"))\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_pred_train = voting.predict_proba(x_train)[:,1]\n",
    "y_pred_valid = voting.predict_proba(tfidf.transform(X_valid))[:,1]\n",
    "\n",
    "train_score = roc_auc_score(y_train, y_pred_train)\n",
    "valid_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(f'Stats train score: {train_score}')\n",
    "print(f'Stats valid score: {valid_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Approach VotingClassifier (Johnson's Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=.2)\n",
    "x_train = feature_selection_part3(X_train, True)\n",
    "x_valid = feature_selection_part3(X_valid, False)\n",
    "y_train = LabelEncoder().fit_transform(Y_train)\n",
    "y_valid = LabelEncoder().fit_transform(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_name = 'x_train_last_dance.pickle'\n",
    "pickle.dump(x_train, open(file_name, \"wb\"))\n",
    "file_name = 'x_valid_last_dance.pickle'\n",
    "pickle.dump(x_valid, open(file_name, \"wb\"))\n",
    "file_name = 'y_train_last_dance.pickle'\n",
    "pickle.dump(y_train, open(file_name, \"wb\"))\n",
    "file_name = 'y_valid_last_dance.pickle'\n",
    "pickle.dump(y_valid, open(file_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pickle.load(open('x_train_last_dance.pickle', 'rb'))\n",
    "x_valid = pickle.load(open('x_valid_last_dance.pickle', 'rb'))\n",
    "y_train = pickle.load(open('y_train_last_dance.pickle', 'rb'))\n",
    "y_valid = pickle.load(open('y_valid_last_dance.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Word Training...\n",
      "Begin Evaluation...\n",
      "Stats train score: 0.6601329938005939\n",
      "Stats valid score: 0.5910334167486859\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model_v3 = XGBClassifier(n_estimators=400, learning_rate=0.0025, max_depth=6)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "model_v3 = model_v3.fit(x_train, y_train)\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_pred_train = model_v3.predict_proba(x_train)[:,1]\n",
    "y_pred_valid = model_v3.predict_proba(x_valid)[:,1]\n",
    "\n",
    "train_score = roc_auc_score(y_train, y_pred_train)\n",
    "valid_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(f'Stats train score: {train_score}')\n",
    "print(f'Stats valid score: {valid_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Word Training...\n",
      "Begin Evaluation...\n",
      "Stats train score: 0.740397027506228\n",
      "Stats valid score: 0.5842086655466674\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=350, max_depth=8, max_features=0.5)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "voting = model.fit(x_train, y_train)\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_pred_train = model.predict_proba(x_train)[:,1]\n",
    "y_pred_valid = model.predict_proba(x_valid)[:,1]\n",
    "\n",
    "train_score = roc_auc_score(y_train, y_pred_train)\n",
    "valid_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(f'Stats train score: {train_score}')\n",
    "print(f'Stats valid score: {valid_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Word Training...\n",
      "Begin Evaluation...\n",
      "Stats train score: 0.5951716816776015\n",
      "Stats valid score: 0.5936696385567399\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=200, learning_rate=0.01, max_depth=3, random_state=0)\n",
    "\n",
    "print('Begin Word Training...')\n",
    "voting = model.fit(x_train, y_train)\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_pred_train = model.predict_proba(x_train)[:,1]\n",
    "y_pred_valid = model.predict_proba(x_valid)[:,1]\n",
    "\n",
    "train_score = roc_auc_score(y_train, y_pred_train)\n",
    "valid_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(f'Stats train score: {train_score}')\n",
    "print(f'Stats valid score: {valid_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Voting Training...\n",
      "Begin Evaluation...\n",
      "Stats train score: 0.6264490999002064\n",
      "Stats valid score: 0.5958093601449124\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model_v1 = XGBClassifier(n_estimators=400, learning_rate=0.002, max_depth=6)\n",
    "model_v3 = GradientBoostingClassifier(n_estimators=200, learning_rate=0.01, max_depth=3, random_state=0)\n",
    "\n",
    "voting = VotingClassifier(estimators=[('v1', model_v1), ('v3', model_v3)], voting='soft')\n",
    "\n",
    "print('Begin Voting Training...')\n",
    "voting = voting.fit(x_train, y_train)\n",
    "\n",
    "print('Begin Evaluation...')\n",
    "y_pred_train = voting.predict_proba(x_train)[:,1]\n",
    "y_pred_valid = voting.predict_proba(x_valid)[:,1]\n",
    "\n",
    "train_score = roc_auc_score(y_train, y_pred_train)\n",
    "valid_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "print(f'Stats train score: {train_score}')\n",
    "print(f'Stats valid score: {valid_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      " Id                                                          27643\n",
      "Page content    <html><head><div class=\"article-info\"><span cl...\n",
      "Name: 0, dtype: object\n",
      "(11847,)\n",
      "\n",
      "Id:\n",
      " 27643\n",
      "Content:\n",
      " <html><head><div class=\"article-info\"><span class=\"byline \"><a href=\"/author/sam-laird/\"><img alt=\"2016%2f09%2f15%2f63%2fhttpsd2mhye01h4nj2n.cloudfront.netmediazgkymde1lza2.9814b\" class=\"author_image\" src=\"http://i.amz.mshcdn.com/-qaMPB8aiQeIaoBhqlU0OLjA07A=/90x90/2016%2F09%2F15%2F63%2Fhttpsd2mhye01h4nj2n.cloudfront.netmediaZgkyMDE1LzA2.9814b.jpg\"/></a><span class=\"author_name\">By <a href=\"/author/sam-laird/\">Sam Laird</a></span><time datetime=\"Mon, 09 Sep 2013 19:47:02 +0000\">2013-09-09 19:47:02 UTC</time></span></div></head><body><h1 class=\"title\">Soccer Star Gets Twitter Death Threats After Tackling One Direction Member</h1><figure class=\"article-image\"></figure><article data-channel=\"entertainment\"><section class=\"article-content\"> <div class=\"shift-to-hero\"> <p><iframe allowfullscreen=\"\" frameborder=\"0\" height=\"360\" src=\"https://www.youtube.com/embed/8L0lUoJ2BCY?enablejsapi=1&amp;\" width=\"640\"></iframe></p> <script src=\"http://a.amz.mshcdn.com/assets/lib/aab-7ce243b38b9cc2caec816aff811d3153.js\" type=\"text/javascript\"></script> </div> <p>Note to humanity: One Direction fandom ain't nothin' to mess with. </p> <p>A British <a href=\"http://mashable.com/category/soccer/\">soccer</a> star learned this hard way over the weekend when the pop band's fanboys and fangirls brought a ruckus to his <a href=\"http://mashable.com/category/twitter/\">Twitter</a> feed, tweeting him a fusillade of death threats following an incidental run-in Sunday during a charity soccer match.</p> <div class=\"see-also\"><p>See also: <a href=\"http://mashable.com/2013/07/30/gq-one-direction-death-threats/\">GQ Gets Twitter Death Threats for One Direction Magazine Covers</a></p></div> <p>Gabriel Agbonlahor is the all-time leading goal-scorer for English Premier League side Aston Villa. He participated in a charity match honoring a former player now battling leukemia. Also playing: Louis Tomlinson, one-fifth of the British-Irish boy band <a href=\"http://mashable.com/category/one-direction/\">One Direction</a>. </p> <p>Tomlinson was dribbling the ball about midway through Sunday's match when the pro striker Agbonlahor swooped in to knock it away from the tiny amateur. A clean play, to be sure, but there was some incidental contact as well, causing Tomlinson to collapse on the grass like a wounded deer. Then he limped off and promptly vomited into his hand, <a href=\"http://www.youtube.com/watch?v=19AADH8tcBM\" target=\"_blank\">as shown here</a>. </p> <p>Tomlinson and Agbonlahor shared a moment as he left the field and all seemed fine on the pitch. On Twitter, however, things were just getting started: One Direction's fan base exploded in a fit of mass rage, threatening to end Agbonlahor in all sorts of unpleasant ways. To whit, here are just a few examples, with some mildly NSFW language:</p> <blockquote class=\"twitter-tweet\"> <p>WHOEVER PUSHED LOUIS DOWN I WILL FIND YOU AND PUSH YOU OFF THE GODDAMN EMPIRE STATE BUILDING!</p> <p>— J a m R e y e s™ (@ItsJamReyes) <a href=\"https://twitter.com/ItsJamReyes/statuses/376714629201674241\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <blockquote class=\"twitter-tweet\"> <p>this is how Louis hurt his knee, I will find that guy, and kill him. <a href=\"https://t.co/3tbsGCT1bq\" target=\"_blank\">https://t.co/3tbsGCT1bq</a></p> <p>— PROUD OF LOUIS (@LatestOf1D) <a href=\"https://twitter.com/LatestOf1D/statuses/376715533560733696\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <blockquote class=\"twitter-tweet\"> <p><a href=\"https://twitter.com/gabby_10\" target=\"_blank\">@gabby_10</a> You obviously need Jesus. WE WILL FUCKING FIND YOU AND KILL YOU.</p> <p>— morgan (@NiallUrBasicUgh) <a href=\"https://twitter.com/NiallUrBasicUgh/statuses/376756036389982208\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <blockquote class=\"twitter-tweet\"> <p>fuck off you <a href=\"https://twitter.com/gabby_10\" target=\"_blank\">@gabby_10</a>, how could u touch my precious LOUIS? I WILL KILL U MUTHERFUCKER</p> <p>— Lucho García #51 (@Lois7x) <a href=\"https://twitter.com/Lois7x/statuses/376781833716760576\" target=\"_blank\">September 8, 2013</a></p> </blockquote> <p>At least one person even took things a step further, threatening indiscriminate mass murder:</p> <blockquote class=\"twitter-tweet\"> <p>If i ever see louis cry bc he's hurt or sad i will kill the entire human race by myself</p> <p>— karol(arry) (@nuttelarry) <a href=\"https://twitter.com/nuttelarry/statuses/377046130682761216\" target=\"_blank\">September 9, 2013</a></p> </blockquote> <p>This is actually just the latest incident of rabid teenyboppers attacking <a href=\"http://mashable.com/sports/\">sports</a> stars online over perceived slights against their demigod idols of auto-tuned worship, however. </p> <p>If you're scoring at home, for example, you'll recall the time not so long ago that Justin Bieber fans <a href=\"http://mashable.com/2013/05/30/justin-bieber-fans-twitter-eric-dickerson/\">unleashed a digital fury</a> upon an NFL Hall of Famer following criticism of Bieber's alleged reckless driving. But the beef goes both ways, too. Hockey fans on Twitter <a href=\"http://mashable.com/2013/07/09/justin-bieber-stabley-cup/\">flew into a collective rage</a> in July after photos surfaced of Bieber posing in quintessentially Bieber-like fashion with the Stanley Cup. </p> <p><strong><div class=\"bonus-content\">BONUS: <a href=\"http://mashable.com/2013/08/04/one-direction-bullying-office-depot-school-supplies/\">One Direction and Directioners Fuse Online Powers to Battle Bullying</a> </div></strong></p> <section class=\"gallery\" data-display-mode=\"gallery\" data-id=\"11859\" data-slide-title-pos=\"\" data-slug=\"one-directions-anti-bullying-campaign-with-office-depot\"> <header> <h1>One Direction's Anti-Bullying Campaign With Office Depot</h1> </header> <ol class=\"slides\"> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced00048d\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/Un3ZihG1VmZWnvs-VprxzwDgFOI=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2F0Fr6eIUoB6I.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/0Fr6eIUoB6I\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">1. 1D + OD Together Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced00048e\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/oOrgEGOY7kCyASO2sJEuiJmcH4Y=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2F2RuwAne1GRk.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/2RuwAne1GRk\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">2. Harry Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced00048f\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/slMRLhuwNWBmi2WUWOsoChIkbT4=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2FIVQeItw5NHQ.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/IVQeItw5NHQ\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">3. Zayn Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced000490\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/eVdXXGFMQcb8Dd0CZ95pp3etjs4=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2FL6Wdbq6frxY.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/L6Wdbq6frxY\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">4. Niall Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced000491\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/-IEKxbwTTehrbTiUHsXA4gv3L4c=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2F6mvTafur08w.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/6mvTafur08w\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">5. Louis Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> <li class=\"slide\" data-content-source=\"Gallery - Video - Youtube\" data-id=\"522e25c6b589e40ced000492\" data-skip-rerender=\"false\" data-thumb=\"http://i.amz.mshcdn.com/U0yZuGwxU2fTisly0txtcIBENkg=/180x140/http%3A%2F%2Fmashable.com%2Fwp-content%2Fgallery%2Fone-directions-anti-bullying-campaign-with-office-depot%2FUlg966fRbJQ.jpg\"> <figure> <div class=\"aspect-16x9\"><iframe allowfullscreen=\"\" class=\"dnr\" frameborder=\"0\" src=\"http://www.youtube.com/embed/Ulg966fRbJQ\"></iframe></div> </figure> <div class=\"meta\"> <h2 class=\"title\">6. Liam Against Bullying </h2> <div class=\"caption\"></div> <div class=\"credit\"> Video: YouTube, <a href=\"http://www.youtube.com/channel/UCt8Iul9mAsdowXx_xrVNVbA\" target=\"_blank\">OfficialOfficeDepot</a> </div> </div> </li> </ol> </section> <p><em>Image: Ian Gavan/Getty Images for Sony Pictures</em></p> <script>      window._msla=window.loadScriptAsync||function(src,id){if(document.getElementById(id))return;var js=document.createElement('script');js.id=id;js.src=src;document.getElementsByTagName('script')[0].parentNode.insertBefore(js,fjs);}; _msla(\"//platform.twitter.com/widgets.js\",\"twitter_jssdk\");</script> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/entertainment/\">Entertainment</a>, <a href=\"/category/music/\">Music</a>, <a href=\"/category/one-direction/\">One Direction</a>, <a href=\"/category/soccer/\">soccer</a>, <a href=\"/category/sports/\">Sports</a> </footer></body></html>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/test.csv')\n",
    "print(\"Content:\\n\",df.loc[0])\n",
    "\n",
    "Id = df.loc[:, 'Id'].to_numpy()\n",
    "X_test = df.loc[:, 'Page content'].to_numpy()\n",
    "print(X_test.shape)\n",
    "print(\"\\nId:\\n\", Id[0])\n",
    "print(\"Content:\\n\", X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(feature_selection_part3(X_test, False))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_word = word_model.predict_proba(tfidf.transform(X_test))[:,1]\n",
    "y_pred_stats = stats_model.predict_proba(feature_selection_part2(X_test))[:,1]\n",
    "propo = 0.35\n",
    "y_pred = y_pred_word*propo + y_pred_stats*(1-propo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  Popularity\n",
      "0      27643    0.447837\n",
      "1      27644    0.436719\n",
      "2      27645    0.439140\n",
      "3      27646    0.578224\n",
      "4      27647    0.456616\n",
      "...      ...         ...\n",
      "11842  39485    0.641587\n",
      "11843  39486    0.476370\n",
      "11844  39487    0.491440\n",
      "11845  39488    0.418514\n",
      "11846  39489    0.431437\n",
      "\n",
      "[11847 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "output_data = {'Id': Id, 'Popularity': y_pred}\n",
    "output_dataframe = pd.DataFrame(output_data)\n",
    "print(output_dataframe)\n",
    "\n",
    "output_dataframe.to_csv(\"./datasets/y_pred.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3585488978559971 0.7407863004889199\n"
     ]
    }
   ],
   "source": [
    "plt.plot(y_pred)\n",
    "plt.show()\n",
    "print(np.min(y_pred), np.max(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "          Id  Popularity\n",
    "0      27643         0.4\n",
    "1      27644         0.7\n",
    "2      27645         0.4\n",
    "3      27646         0.4\n",
    "4      27647         0.5\n",
    "...      ...         ...\n",
    "11842  39485         0.5\n",
    "11843  39486         0.5\n",
    "11844  39487         0.5\n",
    "11845  39488         0.6\n",
    "11846  39489         0.5\n",
    "\n",
    "[11847 rows x 2 columns]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencie & Load Data\n",
    "- Import necessary library and load csv.\n",
    "### Read & Split Data\n",
    "- Split data into X and y.\n",
    "### Feature Engineering\n",
    "- **Positive / Negative Word & Stemmer**\n",
    "    - We create lists for both positive and negative words. In addition, we create positive and negative stemmer to analyze websites' sentiment.\n",
    "- **Stop Word Stemmer**\n",
    "    - We use LancasterStemmer to tokenize selected features.\n",
    "- **All Vector Feature**\n",
    "    - Features\n",
    "        - Title\n",
    "            - Title from header.\n",
    "        - Data Channel\n",
    "            - Data-Channel from html.\n",
    "        - Author\n",
    "            - Auther name from header and only get their family name. (e.g. Clara Moskowitz => Moskowitz).\n",
    "        - Related Topics\n",
    "            - Topics from footer.\n",
    "        - Time\n",
    "            - We convert `HH:MM` to `HH * 60 + MM`, then convert into `\"morning\"`, `\"noon\"`, `\"afternoon\"` and `\"night\"`.\n",
    "        - Whether Weekend of Not\n",
    "            -  `\"weekday\"` or `\"weekend\"`.\n",
    "        - Word Count\n",
    "            - Calculate total words and convert into `\"wordshort\"`, `\"wordmedium\"` and `\"wordlong`.\n",
    "        - Video + Image Count\n",
    "            - Calculate the number of \\<img>, \\<picture>, \\<figure>, \\<video> and \\<iframe>, then convert into `\"nomedia\"`, `\"fewmedia\"` and `\"manymedia\"`.\n",
    "        - Link + Strong Header Count \n",
    "            - Calculate the number of \\<a\\> and \\<strong\\> and convert into `\"nomedia\"`, `\"fewmedia\"` and `\"manymedia\"`.\n",
    "        - Number Count\n",
    "            - Using regular expression to get the amount of numbers and convert into `\"fewstats\"`, `\"somestats\"`, `\"manystats\"` and `\"lotstats\"`.\n",
    "        - Positive and Negative Words\n",
    "            - Collect words from Positive / Negative words list in html.\n",
    "- **Feature Part 1**\n",
    "    - Features\n",
    "        - Title from `feature_selection_all()`\n",
    "        - Data Channel from `feature_selection_all()`\n",
    "        - Author from `feature_selection_all()`\n",
    "        - Related Topics from `feature_selection_all()`\n",
    "- **Feature Part 2**\n",
    "    - Features\n",
    "        - Time from `feature_selection_all()` but is number\n",
    "        - Weekend\n",
    "            - `0` or `1`\n",
    "        - Word count from `feature_selection_all()` but is number\n",
    "        - Script count\n",
    "            - Calculate the number of \\<script>\n",
    "        - Video + Image count from all but is number\n",
    "        - Link + Strong count from all but is number\n",
    "        - Number count from all but is number\n",
    "        - Positive and Negative count\n",
    "            - len(positive words) + len(negative words)\n",
    "        - Number of `\"?\"` and `\"!\"`\n",
    "        - Title length\n",
    "        - Average word length and unique word rate\n",
    "- **Feature Part 3**\n",
    "    - Features from part2\n",
    "    - Features from Johnson's part2\n",
    "### TF-IDF\n",
    "- We use `TfidfVectorizer` with preprocesser `feature_selection_all()`. However, we don't use IF-IDF in our final approach.\n",
    "### Hashing Vectorizer\n",
    "- We use `HashingVectorizer` with preprocess `feature_selection_all()`. However, we don't use Hashing Vectorizer in out final approach.\n",
    "### Model Training\n",
    "- Basically we test training two model, a `TF-IDF` one and a `feature_selection_part2()` or `feature_selection_part3()` one using different model including `RandomForestClassifier`, `XGBClassifier` and `GradientBoostingClassifier` etc. Then we combine their prediction with different propotion to get a better validation score.\n",
    "### Final Approach\n",
    "- In our final approach, we first discover that training only `feature_selection_part3()` with single model can hit better performance. After testing on many models, we choose VotingClassifier consists XGBClassifier and GradientBoostingClassifier as our final model.\n",
    "### Prediction\n",
    "- Prediction our final csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
